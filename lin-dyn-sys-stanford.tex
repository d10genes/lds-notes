% Created 2011-08-19 Fri 12:09
% BEGIN My Article Defaults
    \documentclass[10pt,letterpaper]{article}
    
    \usepackage[letterpaper,includeheadfoot,top=0.5in,bottom=0.5in,left=0.75in,right=0.75in]{geometry}
    \usepackage[utf8]{inputenc}
    \usepackage[T1]{fontenc}
    \usepackage{hyperref}
    \usepackage{lastpage}
    \usepackage{fancyhdr}
    \usepackage[all]{xy}              %for doing graphics http://en.wikibooks.org/wiki/LaTeX/Creating_Graphics#Xy-pic
    \pagestyle{fancy}
    \renewcommand{\headrulewidth}{1pt}
    \renewcommand{\footrulewidth}{0.5pt}
    
    % For the engineering phasor symbol
    % http://newsgroups.derkeiler.com/Archive/Comp/comp.text.tex/2006-02/msg00895.html
    \makeatletter
    \def\phase#1{{\vbox{\ialign{$\m@th\scriptstyle##$\crcr
    \not\mathrel{\mkern10mu\smash{#1}}\crcr
    \noalign{\nointerlineskip}
    \mkern2.5mu\leaders\hrule height.34pt\hfill\mkern2.5mu\crcr}}}}
    \makeatother
    
    % Default footer
    \fancyfoot[L]{\small \jobname \\ \today}
    \fancyfoot[C]{\small Page \thepage\ of \pageref{LastPage}}
    \fancyfoot[R]{\small \copyright \the\year\  Chris Beard}
    % END My Article Defaults
    
    
\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{fixltx2e}
\usepackage{graphicx}
\usepackage{longtable}
\usepackage{float}
\usepackage{wrapfig}
\usepackage{soul}
\usepackage{textcomp}
\usepackage{marvosym}
\usepackage[integrals]{wasysym}
\usepackage{latexsym}
\usepackage{amssymb}
\usepackage{hyperref}
\tolerance=1000
\usepackage{amsmath}
\providecommand{\alert}[1]{\textbf{#1}}
\begin{document}



\title{Linear Dynamic Systems Notes}
\author{Chris Beard}
\date{19 August 2011}
\maketitle

\setcounter{tocdepth}{3}
\tableofcontents
\vspace*{1cm}
Following notes based on the \href{file://.~/Desktop/Engineering/kiet-ee-downloads/current/ee263_course_reader.pdf}{EE263 Course Reader}. Orgmode $\rightarrow$ \LaTeXe

\begin{center}
\begin{tabular}{ll}
 \href{http://www.stanford.edu/~boyd/ee263/matlab/}{Matlab files}                                       &  \href{file://.~/Dropbox/AK-MBP/edu/systems/linear-algebra-notes.pdf}{Linear Algebra}  \\
 \href{file:///Users/FingerMan/Dropbox/AK-MBP/edu/systems/IntroToLinearDynamicalSys/index.html}{Index}  &                                                                                        \\
\end{tabular}
\end{center}


\section{Lecture 2}
\label{sec-1}

\href{file:///Users/FingerMan/Dropbox/AK-MBP/edu/systems/IntroToLinearDynamicalSys/materials/lsoeldsee263/02-lin-fcts.pdf}{Official Lecture Notes}
\subsection{Interpretation for $y=Ax$}
\label{sec-1_1}

\begin{itemize}
\item $A$ a transformation matrix
\item $y$ an observed measurement, $x$ an unknown
\item $x$ is input, $y$ is output; for rows $i$ and columns $j$ in $A$, ${\bf i \rightarrow y}$, ${\bf j \rightarrow x}$

\begin{itemize}
\item indexed as \bf output, input
\end{itemize}

\item Lower diagonal Matrix should make you expect or have a vague thought about causality.

\begin{itemize}
\item $a_{ij}=0$ for $i<j \Rightarrow y_i$ only depends on $x_1,...,x_i$
\item $A$ is diagonal; output only depends on input
\end{itemize}

\item Sparcity pattern (block of zeroes) in a matrix should have you wonder why\ldots{}usually not coincidental
\item $A_{35}$ positive $\Rightarrow x_5$ increased corresponds to an increase in 3rd output, $y_3$
\end{itemize}
\subsection{Example in notes}
\label{sec-1_2}
\subsubsection{Linear elastic structure}
\label{sec-1_2_1}

\begin{itemize}
\item $a_{11}$ probably positive

\begin{itemize}
\item $x_1$ input gives positive push to $y_1$ output
\end{itemize}

\end{itemize}
\subsubsection{Force/Torque on rigid body}
\label{sec-1_2_2}

\begin{itemize}
\item Net torque/force on body is linearly related to force inputs
\end{itemize}
\subsubsection{Thermal System}
\label{sec-1_2_3}

\begin{itemize}
\item despite normally needing a Poisson equation, the steady state heat of the different locations can be represented as a linear system $Ax=y$
\end{itemize}
\section{Lecture 3}
\label{sec-2}
\subsection{Review of Linearization (affine approximation)}
\label{sec-2_1}

  \begin{enumerate}
  \item  If $f: {\bf R}^{n} \rightarrow {\bf R}^{m} $ is differentiable at $x_{0} \in {\bf R} ^{n}$, then
  $x$ near $x_{0} \Rightarrow f(x)$ very near $f(x_{0}) + D f(x_{0})(x-x_{0})$
  where
  $$
  Df(x_{0})_{ij}= \frac{\partial f_{i}}{\partial x_{j}} \bigg|_{x_{0}}
  $$
  is the derivative (Jacobian) matrix.
  
  \item with $y=f(x), y_{0}=f(x_{0})$, define `input deviation' $\delta x := x-x_{0}$, `output deviation' $\delta y:= y-y_{0}$
  \item then we have $\delta y \approx Df(x_{0})\delta x$
  \subitem i.e., we get a linear function for looking at how the output changes with small changes in the input
  \item When deviations are small, they are approximately related by a linear function
  
  \end{enumerate}
\subsubsection{Good intuition range-finding problem}
\label{sec-2_1_1}
\subsection{Intepretations of $Ax=y$}
\label{sec-2_2}

\begin{itemize}
\item Scaled combination of columns in $A$

  $A=[ a_1 a_{2} ... a_{n}] \rightarrow y=x_{1}a_{1}+x_{2}a_{2}+...+x_{n}a_{n}$
\item Looking at the rows: the multiplication is the inner product of rows and vector $x$
\item I/O ordering is backwards in control; in $A_{ij}$, $j$ refers to input and $i$ refers to output

\begin{itemize}
\item indexing is likewise backwards in block diagram indexing
\end{itemize}

\end{itemize}
  $AB=I$; $\tilde a_{i}^{T} \cdot b_{j}=0$ if $i\ne j$, where $\tilde a_{i}$ is the $ith$ row in $A$, and $b_{j}$ is $jth$ column in $B$ 
\subsubsection{Computing $C=AB$}
\label{sec-2_2_1}

How to compute this faster than using formula $c_{ij}=\sum_{k=1} A_{ik} B_{kj}$: have computer break it up into submatrices and do the multiplication on them
\subsubsection{Zero nullspace (in notes)}
\label{sec-2_2_2}

\begin{itemize}
\item if 0 is only element in $\mathcal{N}(A)$

\begin{itemize}
\item $A$ is one-to-one

\begin{itemize}
\item linear transformation doesn't lose information
\end{itemize}

\item columns of $A$ are independent, and basis for their span
\item $A$ has a left inverse $\Rightarrow$ you can use this to undo the transformation and find the input $x$, given the output $y$
\item $|A^T A|\ne0$
\end{itemize}

\end{itemize}
\subsubsection{Interpretations of nullspace}
\label{sec-2_2_3}

supposing $z \in \mathcal{N}(A)$ 
\begin{itemize}
\item Measurements

\begin{itemize}
\item $z$ is undetectable from sensors
\item $x$ and $x+z$ are indistinguishable from sensors: $Ax = A(x+z)$
\item the nullspace characterizes ambiguity in $x$ from measurement $y=Ax$

\begin{itemize}
\item large nullspace is bad
\end{itemize}

\end{itemize}

\item $y=Ax$ is output from input $x$

\begin{itemize}
\item $z$ is input with no result
\item $x$ and $x+z$ have same result
\item the nullspace characterizes freedom of input choice for given result

\begin{itemize}
\item large nullspace is good: more room for optimization because of more input possibilities
\end{itemize}

\end{itemize}

\end{itemize}
\subsubsection{Range}
\label{sec-2_2_4}

Range of $A \in \mathbb{R}^{m\times n}$ defined as $\mathcal{R}(A)=\{Ax | x \in \mathbb{R}^n\} \subseteq \mathbb{R}^m$ 
\begin{itemize}
\item The set of vectors that can be `hit' by linear mapping $y=Ax$
\item span of columns of $A$
\item set of vectors $y$ for which $Ax=y$ has a solution
\item possible sensor measurement

\begin{itemize}
\item in design, you'll want to throw an exception if a measurement is outside the range; the sensor is bad
\item test for a bad 13th sensor: remove 13th row in $A$; if the reduced $y$ is in the range of the reduced matrix, the 13th sensor might not have been bad
\end{itemize}

\end{itemize}
\begin{itemize}

\item Onto matrices\\
\label{sec-2_2_4_1}%
$A$ is `onto' if $\mathcal{R}(A)=\mathbb{R}^m$
\begin{itemize}
\item you can solve $Ax=y$ for any $y$
\item columns of $A$ span $\mathbb{R}^m$
\item $A$ has a right inverse $B$ s.t. $AB=I$

\begin{itemize}
\item can do $ABy=A(By)=y$: you want an $x$ that gives you $y$? Here it is.
\item Design procedure
\end{itemize}

\item rows of $A$ are independent

\begin{itemize}
\item a.k.a., $\mathcal{N}(A^T)=\{0\}$
\end{itemize}

\item $|AA^T|\ne 0$
\end{itemize}

\item Interpretations of range
\label{sec-2_2_4_2}%
\begin{itemize}
\item supposing $v \in \mathcal{R}(A)$

\begin{itemize}
\item $v$ reachable
\item else, not reachable
\end{itemize}

\end{itemize}


\item Inverse\\
\label{sec-2_2_4_3}%
Note: square matrices are impractical for engineering. They don't let you take advantoge of redundant sensors/controllers, or let you build a robust system to take care of broken sensors
\begin{itemize}
\item $A \in \mathbb{R}^{n \times n}$ is invertible or nonsingular if det $A \ne 0$

\begin{itemize}
\item columns of $A$ are basis for $\mathbb{R}^n$
\item rows of $A$ are basis for $\mathbb{R}^n$
\item $y=Ax$ has a unique solution $x$ for every $y \in \mathbb{R}^n$
\item $A$ has left and right inverse $A^{-1} \in \mathbb{R}^{n\times n}$, s.t. $AA^{-1}=A^{-1}A=I$
\item $\mathcal{N}(A)= \{0\}$
\item $\mathcal{R}(A)=\mathbb{R}^n$
\item det $A^T A= |AA^T| \ne 0$
\end{itemize}

\end{itemize}
\begin{itemize}

\item Dual basis intepretation of inverse\\
\label{sec-2_2_4_3_1}%
$a_i$ are columns of $A$, and $\tilde b_i^T$ are rows of $B=A^{-1}$
\begin{itemize}
\item $y=Ax$, column by column, looks like $y=x_1 a_1 + ... + x_n a_n$

\begin{itemize}
\item multiply both sides of $y=Ax$ by $A^{-1}=B$ gives $x=By$
\item so $x_i=\tilde b_i^T y$
\end{itemize}

\end{itemize}

\[
\begin{bmatrix}
  \vdots & \vdots & \vdots & \vdots \\
  a_1    & a_2    & ...    & a_n    \\
  \vdots & \vdots & \vdots & \vdots \\
\end{bmatrix}
\begin{bmatrix}
  x_1      \\
  x_2      \\
  \vdots   \\
  x_n      \\
\end{bmatrix}
=
\begin{bmatrix}
  y_1      \\
  y_2      \\
  \vdots   \\
  y_n      \\
\end{bmatrix}
\]

\[
x=A^{-1} y
\]

\[
\begin{bmatrix}
  x_1      \\
  x_2      \\
  \vdots   \\
  x_n      \\
\end{bmatrix}
=
\begin{bmatrix}
  \cdots & \tilde b^T_1 & \cdots \\
  \cdots & \tilde b^T_2 & \cdots \\
  \cdots & \vdots       & \cdots \\
  \cdots & \tilde b^T_n & \cdots \\
\end{bmatrix}
\begin{bmatrix}
  y_1      \\
  y_2      \\
  \vdots   \\
  y_n      \\
\end{bmatrix}
\]

\[
\begin{bmatrix}
  y_1      \\
  y_2      \\
  \vdots   \\
  y_n      \\
\end{bmatrix}
=
\begin{bmatrix}
x_1 a_1 + ... + x_n a_n
\end{bmatrix}
=
\begin{bmatrix}
(\tilde b^T_1 y) a_1 + ... + (\tilde b^T_n y) a_n
\end{bmatrix}
\]

Beautiful thing:
\[
y=\sum_{i=1}^n (\tilde b_i ^T y) a_i
\]

\end{itemize} % ends low level
\end{itemize} % ends low level
\subsubsection{Rank of matrix}
\label{sec-2_2_5}

Rank of $A \in \mathbb{R}^{m \times n}$ as ${\bf rank}(A)= {\bf dim} \mathcal{R}(A)$
\begin{itemize}
\item ${\bf rank} (A)= {\bf rank} (A^T)$
\item ${\bf rank} (A)$ is maximum number of independent columns or rows of $A$: ${\bf rank} (A) \le {\bf min} (m,n)$
\item ${\bf rank} (A)+ {\bf dim} \mathcal{N}(A)=n$
\end{itemize}
\begin{itemize}

\item Conservation of degrees of freedom (dimension)
\label{sec-2_2_5_1}%
\begin{itemize}
\item ${\bf rank} (A)$ is dimension of set `hit' by mapping $y=Ax$
\item ${\bf dim} \mathcal{N}(A)$ is dimension of set of $x$ `crushed' to zero by $y=Ax$
\end{itemize}
\begin{itemize}

\item Example
\label{sec-2_2_5_1_1}%
\begin{itemize}
\item $A \in \mathbb{R}^{20 \times 10} {\bf rank} (A)=8$

\begin{itemize}
\item you can do 8 dimensions worth of stuff
\item 10 knobs, 2 redundant knobs, which is ${\bf dim} \mathcal{N}(A)=2$
\end{itemize}

\end{itemize}
\end{itemize} % ends low level

\item Coding interpretation of rank
\label{sec-2_2_5_2}%
\begin{itemize}
\item rank of product: ${\bf rank} (BC) \le {\bf min} \{ {\bf rank} (B), {\bf rank} (C)\}$
\item supposedly really cool stuff based on this
\item low rank matrices let you do fast computations
\end{itemize}
\end{itemize} % ends low level
\subsubsection{Various wrap-up items}
\label{sec-2_2_6}
\begin{itemize}

\item RMS\\
\label{sec-2_2_6_1}%
\[
{\bf rms} (x) = \left( \frac{1}{n} \sum^n _{i=1} \right) ^{1/2} = \frac{\| x \|}{\sqrt{n}} 
\]

\item Inner product\\
\label{sec-2_2_6_2}%
$\langle x,y \rangle := x_1 y_1 + x_2 y_2 + \cdots + x_n y_n = x ^{T} y$ 
\begin{itemize}
\item intepretation of inner product signs:
\item $x ^{T} y > 0$: acute; roughly point in same direction
\item $x ^{T} y > 0$: obtuse; roughly point in opposite direction
\end{itemize}

\item Orthonormal set of vectors
\label{sec-2_2_6_3}%
\begin{itemize}
\item set of $k$ vectors $u_1, u_2, ..., u_k \in \mathbb{R}^{n}$ orthonormal; $U= [u_1 \cdots u_k]$
\item $U^T U= I_k \leftrightarrow$ set of column vectors of $U$ are orthonormal
\item ${\bf warning}$: $U U ^{T} \ne I_k$ if $k<n$

\begin{itemize}
\item say $U$ is $10\times 3$, $U^T$ is $3 \times 10$, rank of $U$ is 3 $\Rightarrow$ rank of $UU^T$ is at most 3
\item but $UU^T$ will be a $10\times 10$ matrix, so it can't be the identity matrix
\end{itemize}

\end{itemize}

\end{itemize} % ends low level
\section{Lecture 5}
\label{sec-3}

A good source for more on orthogonality at \href{http://www.math.umn.edu/~olver/aims_/qr.pdf}{University of Minnesota}
\subsection{Geometric properties of orthonormal vectors}
\label{sec-3_1}

\begin{itemize}
\item columns of $U$ are ON $\Rightarrow$ mapping under $U$ preserves distances

\begin{itemize}
\item $w=Uz \Rightarrow \|w \| = \| z \|$
\end{itemize}

\item Also preserves inner product
\item Also preserves angles
\item Something like a rigid transformation
\end{itemize}
\subsection{Orthonormal basis for $\mathbb{R}^{n}$}
\label{sec-3_2}

\begin{itemize}
\item if there are $n$ orthonormal vectors (remember, with dimension $n$), it forms an orthonormal basis for $\mathbb{R}^{n}$
\item $U^{-1}=U^T$

\begin{itemize}
\item \fbox{$U^T U=I \Leftrightarrow U$'s column vectors form an orthonormal basis for $\mathbb{R}^{n}$}
\item $\displaystyle \sum _{i=1} ^{n} u_i u_i^T = I \in \mathbb{R}^{n \times n}$ (known as a dyad, or outer product; inner products reverses the two and gives a scalar, outer gives a matrix)
\item outer products take 2 vectors, possibly of different sizes, and multiplies every combination of elements one with another
\end{itemize}

\end{itemize}
\subsection{Expansion in orthonormal basis}
\label{sec-3_3}

\begin{itemize}
\item $U$ orthogonal $\Rightarrow x=UU^T$
\item $\displaystyle x= \sum ^{n} _{i=1} \left( u ^{T} _{i} x\right) u _{i}$

\begin{itemize}
\item because $U^TU=I$, the thing in sum is really $u_i u_i^T x$
\item $u_i^T x$ is really a scalar, so this can be moved to the front of $u_i$, giving our result
\item This says $x$ is a linear combination of $u_i$'s
\end{itemize}

\end{itemize}
\subsection{Gram-Schmidt procedure}
\label{sec-3_4}

\begin{itemize}
\item $a_1, ..., a_k \in \mathbb{R}^{n}$ are LI; G-S finds ON vectors $q_1,..., q_k$ s.t. $$ {\bf span} (a_1,...,a_r)= {\bf span} (q_1,...,q_r)$$ for $r \le k$
\item so $q_1, ..., q_r$ is an ON basis for span($a_1, ...,a_r$)
\item Basic method: orthogonalize each vector wrt the previous ones, then normalize result

\begin{enumerate}
\item $\tilde q_1 = a_1$
\item normalize: $q_1 = \tilde q_1/ \|\tilde q_1 \|$
\item remove $q_1$ component from $a_2$: $\tilde q_2 = a_2 - (q_1^T a_2) q_1$
\item normalize $q_2$
\item remove $q_1, q_2$ components: $\tilde q_3= a_3 - (q_1^T a_3) q_1 - (q_2^T a_3)q_2$
\item normalize $q_3$
\end{enumerate}

\item $a_i= (q_1^T a_i) q_1 + (q_2^T a_i)q_2 + \cdots + (q_{i-1}^T a_i)q_{i-1} + \| \tilde q_i \| q_i$

\begin{itemize}
\item $= r_{1i} q_1 + r_{2i} q_2 + \cdots + r_{ii} q_i$ ($r_{ii} \ge 0$ is the length of $\tilde q_i$)
\end{itemize}

\end{itemize}
\subsection{$QR$ decomposition}
\label{sec-3_5}

This can be written as $A=QR$, where $A \in \mathbb{R}^{n \times k}, Q \in \mathbb{R}^{n \times k} , R \in \mathbb{R}^{k\times k}$

\[
\begin{bmatrix}
  a_1 & a_2 & \cdots & a_k \\
\end{bmatrix}
=
\begin{bmatrix}
  q_1 & q_2 & \cdots & q_k \\
\end{bmatrix}
\begin{bmatrix}
  r_{11} & r_{12} & \cdots & r_{1k} \\
         0 & r_{22} & \cdots & r_{2k} \\
    \vdots & \vdots   & \ddots & \vdots   \\
         0 & 0        & \cdots & r_{kk} \\
\end{bmatrix}
\]
\begin{itemize}
\item $R$ triangular because computation of $a_i$ only involves up to $q_i$

\begin{itemize}
\item a sort of causality, since you can calculate $q_7$ without seeing $q_8$
\end{itemize}

\item Columns of $Q$ are ON basis for $\mathcal{R}(A)$
\end{itemize}
\subsection{General Gram Schmidt procedure (`rank revealing QR algorithm')}
\label{sec-3_6}

\begin{itemize}
\item Basically the same, but if one of the $\tilde q_i$'s is zero (meaning $a_i$ is dependent on previous $a$ vectors), then just go to the next column
\item referring to notes, upper staircase notation shows which vectors are dependent on previous ones (columns without the x's)

\begin{itemize}
\item entries with x are `corner' entries
\end{itemize}

\end{itemize}
\subsection{Applications}
\label{sec-3_7}

\begin{itemize}
\item check if $b \in {\bf span} (a_1, a2, ..., a_k)$
\item Factorize matrix $A$
\end{itemize}
\subsection{Least Squares Approximation}
\label{sec-3_8}

\begin{itemize}
\item Overdetermined linear equation (tall, skinny, more equations than unknowns, dimensionally redundant system of equations)
\end{itemize}
\section{Lecture 6}
\label{sec-4}

On skinny, full rank matrices
\subsection{Overdetermined equations}
\label{sec-4_1}

\begin{itemize}
\item Skinny, more equations than unknowns
\item Given $y=Ax, A\in \mathbb{R}^{m\times n}$, a randomly-chosen $y$ in $\mathbb{R}^{m}$ has 0 probability of being in the range of $A$
\item To $approximately$ solve for $y$, minimize norm of error (residual) $r=Ax-y$
\item find $x=x _{ls}$ (least squares approx.) that minimizes $\|r\|$
\end{itemize}
\subsection{Least Squares `Solution'}
\label{sec-4_2}

\begin{itemize}
\item square $\|r\|$, get expansion, set gradient wrt $x$ equal to zero
\item \fbox{$x _{ls} = (A ^{T} A) ^{-1} A ^{T} y$ } $=B_{ls} y$ (linear operation)
\item $A ^{T} A$ should be invertible, square, full rank
\item $(A ^{T} A)^{-1} A ^{T}$ is a generalized inverse (is only inverse for square matrices, though)

\begin{itemize}
\item Also known as the $A^\dagger$, `pseudo-inverse'
\item Which is a left inverse of $A$
\end{itemize}

\end{itemize}
\subsection{Projection on $\mathcal{R}(A)$}
\label{sec-4_3}

$Ax _{ls}$ is the point closest to $y$ (i.e., projection of $y$ onto $\mathcal{R}(A)$)
\begin{itemize}
\item $A x _{ls} = {\bf proj} _{ \mathcal{R}(A)} (y)= \left(A(A ^{T} A) ^{-1} A ^{T} \right)y$
\end{itemize}
\subsection{Orthogonality principle}
\label{sec-4_4}

The optimal residual is orthogonal to $C(A)$
\begin{itemize}
\item $r = A x _{ls} -y = (A(A ^{T} A) ^{-1} A ^{T} -I)y$ orthogonal to $C(A)$
\item $\langle r, Az \rangle = y ^{T} (A(A ^{T} A) ^{-1} A ^{T} -I) ^{T} Az = 0$ for all $z \in \mathbb{R}^{n}$
\end{itemize}
\subsection{Least-squares via $QR$ factorization}
\label{sec-4_5}

$A$ is still skinny, full rank
\begin{itemize}
\item Factor as $A=QR$; $Q^TQ=I_n, R \in \mathbb{R}^{n\times n}$ upper triangular, invertible
\item pseudo-inverse: $(A ^{T} A) ^{-1} A ^{T} = R ^{-1} Q ^{T} \Rightarrow$ \fbox{$R ^{-1} Q ^{T} y = x _{ls}$}
\item Pretty straight-forward
\item Matlab for least squares approximation
\begin{verbatim}
     xl = inv(A' * A)*A'y; # So common that has shorthand in MATLAB
     xl = A\y;                 # Works for non-skinny matrices, may do unexpected things
\end{verbatim}

\end{itemize}
\subsection{Full $QR$ factorization}
\label{sec-4_6}

\begin{itemize}
\item $A= \begin{bmatrix}Q_1 & Q_2 \end{bmatrix} \begin{bmatrix} R_1 \\ 0 \end{bmatrix}$

\begin{itemize}
\item New $Q$ is square, orthogonal matrix; $R_1$ is square, upper triangular, invertible
\end{itemize}

\item Remember, multiplying by orthogonal matrix doesn't changet the norm:

\begin{itemize}
\item $\| A x-y \| ^{2} = \| R_1 x - Q ^{T} _{1} y \|^2 + \| Q ^{T} _{2} y \| ^{2}$
\item Find least squares approximation with $x _{ls} = R ^{-1} _{1} Q ^{T} _{1} y$ (zeroes first term)
\end{itemize}

\end{itemize}
\subsection{Applications for least squares approximations}
\label{sec-4_7}

\begin{itemize}
\item if there is some noise $v$ in $y = Ax+v$

\begin{itemize}
\item you can't reconstruct $x$, but you can get close with the approximation
\end{itemize}

\item Estimation: choose some $\hat x$ that minimizes $\| A \hat x - y\|$, which is the deviation between the think we observed, and what we would have observed in the absence of noise
\end{itemize}
\subsection{BLUE: Best linear unbiased estimator}
\label{sec-4_8}

\begin{itemize}
\item $A$ still full rank and skinny; have a `linear estimator' $\hat x= By$ ($B$ is fat)

\begin{itemize}
\item $\hat x = B(Ax+v)$
\end{itemize}

\item Called unbiased if there is no estimation error when there's no noise; the estimator works perfectly in the absence of noise

\begin{itemize}
\item if $v=0$ and $BA=I$; $B$ is left inverse/perfect reconstructor
\end{itemize}

\item Estimation error uf unbiased linear estimator is $x- \hat x= sBv$, so we want $B$ to be small and $BA=I$; small means error isn't sensitive to the noise
\item The pseudo-inverse is the smallest left inverse of $A$:

\begin{itemize}
\item $A ^{\dagger} = (A ^{T} A) ^{-1} A ^{T}$
\item $\displaystyle \sum _{i,j} B ^{2} _{ij} \ge \sum _{i,j} A _{ij} ^{\dagger 2}$
\end{itemize}

\end{itemize}
\subsection{Range-finding example}
\label{sec-4_9}

\begin{itemize}
\item Find ranges to 4 beacons from an unknown position $x$
\item $y = - \begin{bmatrix}
           k _{1} ^{T} \\ k _{2 } ^{T} \\ k _{3} ^{T} \\ k _{4} ^{T} 
         \end{bmatrix} x + v$
\item actual position $x=(5.59, 10.58)$; measurement $y=(-11.95, -2.84, -9.81, 2.81)$

\begin{itemize}
\item these numbers aren't consistent in $Ax=y$, since there's also the error; there is no such $x$ value that can give this $y$ value
\end{itemize}

\item There are 2 redundant sensors (2 more $y$ values than $x$ values); one method for estimating $\hat x$ is `just enough' method: you only need 2 $y$ values; take inverse of top half of $A$ and pad the rest of the matrix with 0's
\item use $\hat x = B _{just enough} y = \begin{bmatrix}\begin{bmatrix} k_1 ^{T} \\ k_2 ^{T} \end{bmatrix} ^{-1} \begin{matrix} 0 & 0 \\ 0 & 0 \end{matrix}\end{bmatrix} = \begin{bmatrix}
                                           0 & -1.0 & 0 & 0 \\ -1.12 &   .5 & 0 & 0 \\   
                                     \end{bmatrix} y = \begin{bmatrix} 2.84 \\ 11.9 \end{bmatrix}$
\item Least Squares method: $\hat x A ^{\dagger} y =$ this has a much smaller norm of error
\item Just enough estimator doesn't seem to have good performance\ldots{}unless last two measurements were really off, since JEM only takes 2 measurements into account
\end{itemize}
\subsection{Quantizer example}
\label{sec-4_10}

Super-impressive least squares estimate; more precise than A-D converter
\subsection{Least Squares data-fitting}
\label{sec-4_11}

\begin{itemize}
\item use functions $f_1, f_2, ..., f_n:S \rightarrow \mathbb{R}$ are called regressors or basis functions
\item applications

\begin{itemize}
\item interpolation- , extrapolation, smoothing of data
\end{itemize}

\end{itemize}

\begin{center}
\begin{tabular}{ll}
 Applications                    &                                                                                    \\
\hline
 interpolation                   &  don't have sensors in specific location, but want the temperature                 \\
 extrapolation                   &  get good basis functions for better interpolation                                 \\
 data smoothing                  &  de-noise measurements                                                             \\
 simple, approximate data model  &  Get a million samples, use the data-fitting to get a simple approximate function  \\
\end{tabular}
\end{center}
\subsection{Least-squares polynomial fitting}
\label{sec-4_12}

\begin{itemize}
\item Vandermonde matrix?
\end{itemize}
\section{Lecture 7}
\label{sec-5}
\subsection{Least-squares polynomial fitting, cont'd}
\label{sec-5_1}

\begin{itemize}
\item have data samples $(t_i, y_i), i=1,...,m$
\item fit coefficients $a_i$ of polynomial $p(t)= a_0 + a_1 t + \cdots + a _{n-1} t ^{t-1}$ so that when evaluated at $t_i$ it will give you the associated $y$ value
\item basis functions are $f_j(t)= t ^{j-1}, j=1,...,n$
\item use Vandermonde matrix $A$ (`polynomial evaluator matrix'):
\end{itemize}
\[
A=
\begin{bmatrix}
  1 & t_1    & t_1^2 & ... & t_1^{n-1} \\
  1 & t_2    & t_2^2 & ... & t_2^{n-1} \\
    & \vdots &       &     & \vdots    \\
  1 & t_m    & t_m^2 & ... & t_m^{n-1} \\
\end{bmatrix}
\]
\begin{itemize}
\item side note: use this when you want to fit throughout an interval, use a Taylor series fit if you want it close to a point
\end{itemize}
\subsection{Growing sets of regressors}
\label{sec-5_2}

\begin{itemize}
\item Given ordered set of vectors; find best fit with first vector, then best fit with first and second, then best fit with first three\ldots{}
\item These vectors called \emph{regressors}, or columns
\item Say you have some \emph{master list} $A$ with $n$ columns, and $A ^{(p)}$ will be the matrix with the first $p$ columns of it

\begin{itemize}
\item we want to minimize different sets of $\| A ^{(p)} x-y \|$
\item i.e., project $y$ onto a growing span $\{a_1, a_2, ..., a_p\}$
\end{itemize}

\item Solution for each $p \le n$ given by $x _{ls} ^{(p)} = (A ^{T} _{p} A _{p} )^{-1} A _{p} ^{T} y = R ^{-1} _{p} Q ^{T} _{p} y$

\begin{itemize}
\item In MATLAB, \texttt{A(:,1:p)\textbackslash{}y}, though technically it's faster to do a sort of \texttt{for} loop
\end{itemize}

\item Residual, $\| \sum ^{p} _{i=1} x_i a_i -y \|$ reduces as $p$ (number of columns) increases

\begin{itemize}
\item though it may be same as residual with previous value of $p$ if the optimal $x_1=0$, when $y \perp a_1$
\item if the residual drops 15\% from that of previous value of $p$, you say that $a_1$ explains 15\% of $y$
\end{itemize}

\end{itemize}
\subsection{Least-squares system identification (important topic)}
\label{sec-5_3}

\begin{itemize}
\item measure input, output $u(t), y(t)$ for $t=0,...,N$ of unknown system, and try to get a model of system
\item example: moving average (MA) model with $n$ delays (try to approximate what are the weights $h_i$ for each delay)

\begin{itemize}
\item see equation/matrix in notes, though there are different ways to write it
\item get best answer with LSA
\end{itemize}

\end{itemize}
\subsection{Model order selection}
\label{sec-5_4}

\begin{itemize}
\item how large should $n$ be?
\item the larger, the smaller prediction error on \emph{data used to form model}
\item but at a certain point, predictive ability of model on other I/O data from same system worsens
\item probably best to choose the `knee' on the graph on notes slide for prediction of new data
\end{itemize}
\subsubsection{Cross-validation}
\label{sec-5_4_1}

\begin{itemize}
\item check with new data, only if you're getting small residuals on data you've already seen
\item when $n$ gets too large (greater than $n=10$ on graph), the error with `validation data' actually gets larger
\item this example is ideal, since $n=10$ is the obvious order for the model
\item \textbf{Application note}: in medical, many industries, there's a firm wall between validation data and model-developing data, so someone \emph{else} tests your model
\item in this example, it is known as \emph{overfit} when the validation data error gets larger for $n$ too large
\end{itemize}
\subsection{Growing sets of measurements}
\label{sec-5_5}

\begin{itemize}
\item similar to GSo Regressors, except you add new rows, not columns
\item this would happen if we're estimating a parameter $x$ (which is constant)
\item Solution: $\displaystyle x_{ls} = \left( \sum ^{m} _{i=1} a_i a_i ^{T} \right) ^{-1} \sum ^{m} _{i=1} y_i a_i$
\item new way to think of least squares equation
\end{itemize}
\subsection{Recursive ways to do least squares}
\label{sec-5_6}

\begin{itemize}
\item don't have to re-add for each new measurement

\begin{itemize}
\item i.e., memory is bounded
\item use equation from notes; solution is $x _{ls} (m) = P(m) ^{-1} q(m)$
\end{itemize}

\end{itemize}
\subsection{Fast update algorithm for recursive LS}
\label{sec-5_7}

\begin{itemize}
\item Was a big deal back in the day; somewhat still
\end{itemize}
\subsection{Multi-objective least squares}
\label{sec-5_8}

\begin{itemize}
\item Sometimes you have 2+ objectives to minimize

\begin{itemize}
\item say $J_1 = \| Ax-y\| ^{2}$ (what we've done so far)
\item and $J_2 = \| Fx-g\| ^{2}$
\item these are usually competing (minimize one at cost of other)
\end{itemize}

\item Variable in question is $x \in \mathbb{R}^{n}$
\item Plot in notes shows plot of $(J_1(x_i), J_2(x_i))$
\item Some points are unambiguously worse than others, but there is some ambiguity when $J_1(x_1) < J_1(x_2)$, while $J_2(x_1) > J_2(x_2)$
\item Fix this ambiguity with `weighted-sum objective'
\item $J_1 + \mu J_2 = \| Ax-y\| ^{2} + \mu \| Fx-g\| ^{2}$

\begin{itemize}
\item Say, there's a trade-off between smoothness (no noise) and better fit; $\mu$ can have different dimensions if $J_2$ does
\end{itemize}

\item Use slope of $\mu$ in graph (`indifference curve', in economics) [slide 7-6]
\end{itemize}
\section{Lecture 8}
\label{sec-6}

Multi-objective least-squares
\subsection{Plot of achievable objective pairs}
\label{sec-6_1}

\begin{itemize}
\item if it approximates an L shape (has a `knee'), the knee is usually the obvious optimal location, so least-squares isn't as helpful

\begin{itemize}
\item optimal point isn't very sensitive to $\mu$
\end{itemize}

\item Other extreme: trade-off curve looks linear (negative slope), where it's zero-sum

\begin{itemize}
\item optimal point very sensitive to $\mu$
\item slope commonly called \emph{exchange rate curve}
\end{itemize}

\item In this class, they must be convex curves (cup up/outward)
\item To find Pareto optimal points, minimize $J_1 + \mu J_2 = \alpha$

\begin{itemize}
\item on plot, can have level curves with slope $\mu$
\item Find point on Pareto Optimal Curve that has slope $\mu$
\end{itemize}

\end{itemize}
\subsection{Minimizing weighted-sum objective}
\label{sec-6_2}

\begin{itemize}
\item note: norm-squared of a stacked vector is norm-square of the top+norm-square of bottom
\end{itemize}
$$J_1+\mu J_2 = \| Ax - y \| ^{2} + \mu \| Fx - g \| ^{2} = \left\|
\begin{bmatrix}
  A           \\
  \sqrt \mu F \\
\end{bmatrix}
x-
\begin{bmatrix}
  y \\
  \sqrt \mu g \\
\end{bmatrix}
\right\| ^{2} 
$$
\[
= \left\| \tilde Ax- \tilde y \right\|
\]
where
\[
\tilde A =
\begin{bmatrix}
  A           \\
  \sqrt \mu F \\
\end{bmatrix}
, \tilde y =
\begin{bmatrix}
  y \\
  \sqrt \mu g \\
\end{bmatrix}
\]
If $\tilde A$ is full rank,
\begin{eqnarray}
x &=& \left( \tilde A ^{T} \tilde A \right)^{-1} \tilde A ^{T} \tilde y \\
  &=& ( A ^{T} A + \mu F ^{T} F) ^{-1} (A ^{T} y + \mu F ^{T} g)
\end{eqnarray}
Note: to plot the tradeoff curve, calculate the minimizer $x_\mu$, and plot the resulting pairs $(J_1, J_2)$ 
In MATLAB, \texttt{[A; sqrt(mu) * F]\textbackslash{}[y;sqrt(mu) * g]}
\subsection{Example: frictionless table}
\label{sec-6_3}

\begin{itemize}
\item $y$ is final position at $t=10$; $y=a ^{T} x$, $a\in \mathbb{R}^{10}$
\item $J_1 = (y-1) ^{2}$, (final position difference from $y=1$ squared)
\item $J_2 = \|x\| ^{2}$ sum of force squares
\item Q: Why do we often care about sum of squares? A: \textbf{It's easy to analyze} (not necessarily because it corresponds to energy)

\begin{itemize}
\item max $| x_i |$ corresponds to maximum thrust
\item $\sum |x_i|$ corresponds to fuel use
\end{itemize}

\item Optimal tradeoff curve is quadratic
\end{itemize}
\subsection{Regularized least-squares}
\label{sec-6_4}

\begin{itemize}
\item famous example of multi-objective least squares

\begin{itemize}
\item second $J$ term is simply $J_2 = \|x\|$, though first is the same: $J_1 = \|Ax-y\| ^{2}$
\end{itemize}

\item Tychonov regularization works for \emph{any} $A$

\begin{itemize}
\item \emph{regularized} least-squares solution: \fbox{$x_\mu = (A ^{T} A + \mu I) ^{-1} A ^{T} y$}, for $F=I, g=0$
\end{itemize}

\end{itemize}
Show $(A ^{T} A + \mu I)$ is invertible, no matter what size/values of $A$ (assuming $\mu > 0$ ): 
If this is \emph{not} invertible (singular), it means some nonzero vector $z$ gets mapped to zero ($z \in \mathcal{N}(A)$)
\begin{eqnarray}
(A ^{T} A + \mu I) z= 0, z \ne 0 \\
z ^{T} (A ^{T} A + \mu I) z = 0 \text{ since } z^T \vec{0} =0 \\
z ^{T} A ^{T} A z + \mu z ^{T} z = 0 \\
\| A z \| ^{2} + \mu \| z \| ^{2} = 0 \\
z = \vec{0} 
\end{eqnarray}
So, $z$ can only be zero, meaning $\mathcal{N}(A) = \{0\} \Rightarrow (A ^{T} A -\mu I)$ is invertible. This is also why $\mu$ must be positive.
Or, you know it's invertible, since it is full rank (and skinny) when you stack $\mu I$ below it (see definition of $\tilde A$).
\begin{itemize}
\item Application of Regularized least-squares

\begin{itemize}
\item estimation/inversion
\item $Ax-y$ is sensor residual
\item prior information that $x$ is really small
\item or, model only accurate for small $x$
\item Tychonov solution trades off sensor fit and size of $x$
\end{itemize}

\item Image processing example

\begin{itemize}
\item Laplacian regularization

\begin{itemize}
\item image reconstruction problem
\end{itemize}

\item $x$ is vectorized version of image
\item $\|A x - y\| ^{2}$ is difference from real image
\item Want new objective to minimize roughness

\begin{itemize}
\item vector $Dx$ (from new matrix $D$) which has difference between neighboring pixels as elements

\begin{itemize}
\item $D_v x$ measures vertical difference
\item $D_h x$ measures horizontal difference
\item Nullspace is vector where there is no variation between pixels
\end{itemize}

\end{itemize}

\item minimize $\|A x-y\| ^{2} + \mu \| [D_h x \; D_v x]^{T} \| ^{2}$

\begin{itemize}
\item if $\mu$ is turned way up, it'll be all smoothed out
\item if you care about total size of image, you can add another parameter $\lambda$: $\|A x-y\| ^{2} + \mu \| [D_h x \; D_v x]^{T} \| ^{2} + \lambda \|x\| ^{2}$
\end{itemize}

\end{itemize}

\end{itemize}
\subsection{Nonlinear least squares (NLLS) problem}
\label{sec-6_5}

\begin{itemize}
\item find $x\in \mathbb{R}^{n}$ that minimizes $\displaystyle \| r(x) \| ^{2} = \sum ^{m} _{i=1} r _{i} (x) ^{2}$
\item $r(x)$ is vector of residuals; $r(x)= Ax-y \Rightarrow$ problem reduces to linear least squares problem
\item in general, can't \textbf{really} solve a NLLS problem, but can find good heuristics to get a locally optimal solution
\end{itemize}
\subsection{Gauss-Newton method for NLLS}
\label{sec-6_6}

\begin{itemize}
\item Start guess for $x$
\item Loop

\begin{itemize}
\item linearize $r$ near current guess
\item new guess is linear LS solution, using linearized $r$
\item if convergence, stop
\end{itemize}

\item Linearize?

\begin{itemize}
\item Jacobian: $(Dr) _{ij} = \partial r _{i} / \partial x_j$
\item Linearization: $r (x) \approx r(x ^{(k)}) + Dr(x ^{(k)} ) (x-x ^{(k)} )$
\item Set this linearized approximation equal to $r(x) \approx A ^{(k)} x-b ^{(k)}$

\begin{itemize}
\item $A ^{(k)} = Dr(x ^{(k)})$
\item $b ^{(k)} = Dr (x ^{(k)}) x ^{(k)} -r(x ^{(k)})$
\end{itemize}

\item See rest in notes
\item At $k$ th iteration, approximate NLLS problem by linear LS problem:

\begin{itemize}
\item $\| r (x) \| ^{2} \approx \left\| A ^{(k)} x-b ^{(k)} \right\| ^{2}$

\begin{itemize}
\item if you wanna make this really cool add a $\mu \|x-x ^{(k)} \| ^{2}$ term on RHS
\item called a `trust region term';
\item first (original) part says to minimize sum of squares for \emph{model}
\item trust region term says `but don't go far from where you are now'
\end{itemize}

\end{itemize}

\end{itemize}

\item Could also linearize without calculus; works really well

\begin{itemize}
\item See `particle filter'
\end{itemize}

\end{itemize}
\subsection{G-N example}
\label{sec-6_7}

\begin{itemize}
\item Nice graph and residual plot
\item As practical matter, good to run simulation several times (with different initial guesses)
\item `exuastive simulation'
\end{itemize}
\subsection{Underdetermined linear equations}
\label{sec-6_8}

\begin{itemize}
\item $A \in \mathbb{R}^{m \times n}, m<n$ ($A$ is fat)
\item more variables than equations
\item $x$ is underspecified
\item For this sectian \textbf{assume $A$ is full rank}
\item Set of all solutions has form $\{x | Ax=y\} = \{x_p + z | z \in \mathcal{N}(A) \}$
\item solution has dim $\mathcal{N}(A)= n-m$ `degrees of freedom'

\begin{itemize}
\item many DOF: good for design (flexibility), bad for estimation (stuff you don't/can't know with available measurements)
\end{itemize}

\end{itemize}
\subsection{Least norm solution}
\label{sec-6_9}

\begin{itemize}
\item \fbox{$x _{ls} = A ^{T} (AA ^{T}) ^{-1} y$}

\begin{itemize}
\item similar to our familiar skinny $A$ version: $x _{ls} = (A^{T} A) ^{-1} A ^{T} y$
\item mnemonic: $(\cdot) ^{-1}$ thing must be square

\begin{itemize}
\item if $A$ skinny, both \$A A $^{T}$ and \$ $A^TA$ could be square (syntactically)
\item semantically, you need the up and down patterns that will form the smallest square, i.e., full rank matrix
\end{itemize}

\end{itemize}

\end{itemize}
\section{Lecture 9 pt 2}
\label{sec-7}

Thank you fucking Suncheon.
\subsection{General norm minimization with equality constraints}
\label{sec-7_1}

\begin{itemize}
\item Problem: \fbox{minimize $\|A x-b\|$ subject to $Cx=d$, with variable $x$}
\item Least squares/least norm are special cases

\begin{itemize}
\item Least norm: set $A=I, b=0$, then you just have norm of $x$ subject to some linear equations
\end{itemize}

\item Same as: minimize $(1/2) \|Ax-b\| ^{2}$ subject to $Cx=d$
\item Lagrangian is\ldots{}long ugly thing\ldots{}look at notes

\begin{itemize}
\item a bit easier to look at block matrix format
\end{itemize}

\end{itemize}
$$
\begin{bmatrix}
  A ^{T} A & C ^{T} \\
  C        & 0      \\
\end{bmatrix}
\begin{bmatrix}
  x       \\
  \lambda \\
\end{bmatrix}
=
\begin{bmatrix}
  A ^{T} b \\
  d        \\
\end{bmatrix}
$$
\begin{itemize}
\item recover least squares (maybe) by eliminating $C$ from matrix (not setting to zero, but only having 1 row/column in first matrix)
\end{itemize}
\subsection{Autonomous linear dynamical systems}
\label{sec-7_2}

``What the class is nominally about''
\begin{itemize}
\item In continuous time, autonomous LDS has form $\dot x = Ax$
\item Solution: $x(t) = e ^{ta} x(0)$
\item $x(t) \in \mathbb{R}^{n}$ is called the state

\begin{itemize}
\item $n$ is state dimension
\end{itemize}

\item $A$ basically maps where you are ($x$) to where you're going ($\dot x$)

\begin{itemize}
\item has units of s$^{-1}$, frequency
\end{itemize}

\item Example illustration: vector fields
\end{itemize}
\subsection{Block diagrams}
\label{sec-7_3}

\begin{itemize}
\item use integrators to express $\dot x =Ax$ instead of differentiators

\begin{itemize}
\item block called `bank of integrators'
\item historically used because of analog, mechanical computers
\end{itemize}

\item notches to express $n$ signals
\end{itemize}
\subsection{Linear circuit example}
\label{sec-7_4}
\section{Lecture 10}
\label{sec-8}

Examples of autonomous linear dynamical systems, $\dot x = Ax$ 
\subsection{Example: Series reaction $A \rightarrow B \rightarrow C$}
\label{sec-8_1}

$$
\dot x=
\begin{bmatrix}
  -k_1 & 0    & 0 \\
  k_1  & -k_2 & 0 \\
  0    & k_2  & 0 \\
\end{bmatrix}
x
$$
\begin{itemize}
\item For second row, first term on rhs of $\dot x_2 = k_1 x_1 - k_2 x_1$ is \emph{buildup}
\item Note: Column sums are 0 implies conservation of mass/materials;
\end{itemize}
\subsection{Discrete time Markov chain}
\label{sec-8_2}

\begin{itemize}
\item $x(t+1) = Ax(t)$
\item $x(t) = A ^{t} x(0)$
\item Given current state, the matrix of \emph{transition probabilities} $P$ will tell you probabilities of the next state, given the current state
\end{itemize}
\subsection{Numerical integration of continuous system}
\label{sec-8_3}

\begin{itemize}
\item for a small time step $h$, find about where you'll be in $h$ seconds b
\item $x(t+h) \approx x(t) + h \dot x(t) = (I + hA) x(t)$
\item problem: when you do it for a long time, error can build up pretty high
\end{itemize}
\subsection{Higher order linear dynamical systems ($\dot x=Ax$)}
\label{sec-8_4}

$x ^{(k)} = A _{k-1} x ^{(k-1)} + \cdots + A _{1} x ^{(1)} + A _{0} x, x(t) \in \mathbb{R}^{n}$
\begin{itemize}
\item define new variable
\end{itemize}
$$
z =
\begin{bmatrix}
  x          \\
  x ^{(1)}   \\
  \vdots     \\
  x ^{(k-1)} \\
\end{bmatrix}
\in \mathbb{R}^{nk},
\dot z =
\begin{bmatrix}
  x ^{(1)}   \\
  \vdots     \\
  x ^{(k)} \\
\end{bmatrix}
=
\begin{bmatrix}
       0 &   I &   0 & \cdots & 0        \\
       0 &   0 &   I & \cdots & 0        \\
  \vdots &     &     &        & \vdots   \\
       0 &   0 &   0 & \cdots & I        \\
     A_0 & A_1 & A_2 & \cdots & A _{k-1} \\
\end{bmatrix}
z
$$
\begin{itemize}
\item `upshift $x$, and zero-pad'
\item $z$ is the state, not $x$
\item in notes, black diagram with chain of integrators
\end{itemize}
\subsection{Example: Mechanical systems}
\label{sec-8_5}

\begin{itemize}
\item Ex: $K _{12}$ is `cross-stiffness', how much stiffness you'd feel at node 1 from node 2
\end{itemize}
\subsection{Linearization near equilibrium point}
\label{sec-8_6}

Equilibrium point corresponds to constant solution ($f(x_e)= 0, x(t)=x_e$)
\begin{itemize}
\item if you start at an equilibrium point, you'll stay there
\item if you start \emph{near} equilibrium point

\begin{itemize}
\item veer off (unstable)
\item go towards equilibrium (stable)
\item something in between
\end{itemize}

\item but, you never stay at an unstable equilibrium position, since equation is really $\dot x = f(x) + w(t)$, where $w(t)$ is noise
\item Near equilibrium point, $\dot{\delta x}(t) \approx Df(x_e) \delta x(t)$, where $D$ is the Jacobian

\begin{itemize}
\item similar to euler forward equation
\end{itemize}

\item Don't fully trust approximations on approximations (but hope they work)
\end{itemize}
\subsection{Example: pendulum linearization}
\label{sec-8_7}

\begin{itemize}
\item $ml ^{2} \ddot{\theta}=-lmg \sin \theta$
\item rewrite as 1st order DE with state $x=[\theta \; \dot \theta] ^{T} = [x_1 \; x_2] ^{T}$:
\end{itemize}
$$
\dot x =
\begin{bmatrix}
  x _{2}          \\
  -(g/l) \sin x_1 \\
\end{bmatrix}
$$ 
\begin{itemize}
\item $\exists$ equilibrium point at $x=0$ (and $\pi$), so we linearize system near $x _{e} =0$, using a Jacobian matrix:
\end{itemize}
$$
\dot{\delta x}=
\begin{bmatrix}
  \frac{\partial x_2}{\partial x_1}               & \frac{\partial x_2}{\partial x_2}               \\
  \frac{\partial}{\partial x_1} \left(-(g/l) \sin x_1 \right)|_{x_1=0} & \frac{\partial}{\partial x_2} (-(g/l) \sin x_1) \\
\end{bmatrix}
\delta x  
=
\begin{bmatrix}
     0 & 1 \\
  -g/l & 0 \\
\end{bmatrix}
\delta x  
$$
\section{Lecture 11}
\label{sec-9}

Solution via Laplace transform and matrix exponential
Remember, we've already overloaded $\dot x =ax$. Now, we'll overload exponentials to apply to matrices $x(t) = e ^{ta} x(0)$.
\subsection{Laplace transform}
\label{sec-9_1}

\begin{itemize}
\item $z: \mathbb{R} _{+} \rightarrow \mathbb{R}^{p\times q}$ (function that maps non-negative real scalars to matrices)
\item Laplace transform: $Z= \mathcal{L}(z)$, defined by $\displaystyle Z(s) = \int _{0} ^{\infty} e ^{-st} z(t) dt$
\item Region of convergence of $Z$ is mostly for confusing students
\item Derivative property: $\mathcal{L}(\dot z) = sZ(s)-z(0)$
\end{itemize}
So, we can use the Laplace transform to solve $\dot x=Ax$. Take Laplace: $sX(s)-x(0)=AX(s)$, rewrite as $(sI-A)X(s) = x(0)$, so $X(s) = (sI-A) ^{-1} x(0)$. Then take the inverse transform: \fbox{$x(t) = \mathcal{L} ^{-1} \left( (sI-A) ^{-1} \right) x(0)$}
\begin{itemize}
\item takes advantage if linearity of the Laplace transform
\item $(sI-A) ^{-1}$ is called the \emph{resolvent} of $A$

\begin{itemize}
\item but not defined for eigenvalues of $A$; $s$, ST det($sI-A$)=0
\end{itemize}

\item \fbox{$\Phi = \mathcal{L} ^{-1} ((sI-A) ^{-1} )$ } is called the \emph{state-transition matrix}, which maps the initial state to state at time $t$: \fbox{$x(t) = \Phi(t)x(0)$ }
\end{itemize}
\subsection{Example: Harmonic oscillator}
\label{sec-9_2}

$$
\dot x =
\begin{bmatrix}
   0 & 1 \\
  -1 & 0 \\
\end{bmatrix}
x
$$ 
\begin{itemize}
\item To solve for $s$, get the resolvent, then apply the Laplacion to it \emph{elementwise}, getting
\end{itemize}
$$
x(t) =
\begin{bmatrix}
  \cos t  & \sin t \\
  -\sin t & \cos t \\
\end{bmatrix}
x(0)
$$
Which is a circular rotation matrix. The solutions to $\dot x = ax$ is $x(t) = e ^{ta} x(0)$
\begin{itemize}
\item $a$ positive: exponential growth
\item $a$ negative: exponential decay
\item $a=0$: constant
\end{itemize}
\subsection{Example: Double Integrator}
\label{sec-9_3}

\begin{itemize}
\item Note, with scalars, $x$ in $\dot x=ax$ grows exponentially in time, and cannot grow linearly, as with matrices (can have a $t$ element in matrix)
\item What is first column of $\Phi(t)$ say? It tells what the state trajectory is if the initial condition was $e_1$ (second column tells what it is if $x(0)= e_2$)
\item First row says the linear combination that $x_1$ is at time $t$ given $x(0)$
\end{itemize}
\subsection{Characteristic polynomial}
\label{sec-9_4}

$\mathcal X(s) = {\bf det} (sI-A)$; called a \emph{monic} polynomial
\begin{itemize}
\item roots of $\mathcal X$ are eigenvalues of $A$, and $\mathcal X$ has real coefficients, so e-values are real or occur in conjugate pairs
\end{itemize}
\subsection{Get eigenvalues of $A$ and poles of resolvent}
\label{sec-9_5}

Use Cramer's rule to get $i,j$ entry:
$$
(-1) ^{i+j} \frac{\text{det} \Delta _{ij}}{\text{det}(sI-A)},
$$
where $\Delta _{ij}$ is $sI-A$ with $j$ th row and $i$ th column deleted. Poles of entries of resolvent \textbf{must} be eigenvalues of $A$.
\subsection{Matrix exponential}
\label{sec-9_6}

How to overload exponentials for matrices; start with $(I-C) ^{-1}= I + C + C ^{2} +$ \ldots{} Series converges if |eigenvalues of $C$ |<1.
Do series expansion of resolvent, then take the Laplacian of the series, which looks like the form for the expansion of $e ^{ta}$ (though square matrices replace scalars). So we end by learning that the state transition matrix, $\Phi(t)$ is the matrix exponential $e ^{tA}$.
\begin{itemize}
\item Many scalar exponential properties don't extend to matrix exponential; with scalars, this is wrong: $e ^{A+B} = e ^{A} e ^{B}$ (unless $A$ and $B$ commute: $AB=BA$)
\item But this is ok: $e ^{-A} = (e ^{A} ) ^{-1}$
\item So, how do you find the matrix exponential:
\end{itemize}
Find $e^A$,
$$
A=
\begin{bmatrix}
  0 & 1 \\
  0 & 0 \\
\end{bmatrix}
$$ 
Found $e ^{tA} = \mathcal L ^{-1} (sI-A) ^{-1}$ in earlier example, so just plug in $t=1$.
\begin{itemize}
\item Matlab: \texttt{expm(A)}, not elementwise \texttt{exp(A)}
\end{itemize}
\subsection{Time transfer property}
\label{sec-9_7}

Summary: for $\dot x = Ax$, $x(t) = \Phi (t)x(0) =$ \fbox{$e ^{tA} x(0)$}. \fbox{The matrix $e ^{tA}$ propagates initial condition into state at time $t$.} Also propagates backward in time if $t<0$.

If given $x(12)$, find $x(0)$ via $e ^{-12A} x(12)$.
\begin{itemize}
\item Can use first order forward Euler approximate state update for small $t$
\item Discretized autonomous LDS: $z(k+1) = e ^{hA} z(k)$ (not an approximation for these equations)
\end{itemize}
\subsection{Application: sampling a continuous time system}
\label{sec-9_8}
\section{Lecture 12}
\label{sec-10}

Piecewise constant system: $A$ is constant for certain intervals of time.
\begin{itemize}
\item Qualitative behavior of $x(t)$

\begin{itemize}
\item Eigenvalues determine (possible) behavior of $x$
\item Can plot eigenvalues on complex axes; like pole plot
\item Can put $x$ in summation form with polynomial coefficient and exponential terms
\end{itemize}

\end{itemize}
\subsection{Stability}
\label{sec-10_1}

\begin{itemize}
\item $\dot x=Ax$ is stable if $e ^{tA} \rightarrow 0$ as $t \rightarrow \infty$

\begin{itemize}
\item means that state $x(t)$ converges to 0 as $t \rightarrow \infty$, no matter $x(0)$
\item all trajectories of $\dot x = Ax$ converge to 0 as $t \rightarrow \infty$
\item $\dot x=Ax$ is stable iff all eigenvalues of $A$ have negative real part
\end{itemize}

\end{itemize}
\subsection{Eigenvectors and diagonalization}
\label{sec-10_2}

\begin{itemize}
\item $\lambda \in \mathbb C$ is an eigenvalue of $A \in \mathbb C ^{n\times n}$ if (characteristic polynomial)
\end{itemize}
$$
\mathcal X(\lambda) = \text{det}(\lambda I-A) = 0
$$
\begin{itemize}
\item i.e., $(\lambda I-A)$ is singular, not invertible, $\mathcal{N}$ not equal to the 0 set
\end{itemize}
Equivalent to:
\begin{itemize}
\item $\exists$ nonzero $v \in \mathbb C ^{n}$ s.t. $(\lambda I -A) v = 0$: \fbox{$Av=\lambda v$} ($v$ is the eigenvector)

\begin{itemize}
\item columns are dependent
\end{itemize}

\item $\exists$ nonzero $w \in \mathbb C ^{n}$ s.t. $w ^{T} (\lambda I -A) = 0$: \fbox{$w^T A=\lambda w ^{T}$} ($w$ is the \emph{left eigenvector})

\begin{itemize}
\item rows are dependent
\end{itemize}

\item real $A$ can still have complex e-pairs
\item $A,\lambda$ real $\Rightarrow$ $\lambda$ is associated with a real $v$
\item conjugate (negate imaginary term of complex number[s])
\item hermitian conjugate (and transpose)
\end{itemize}
\subsection{Scaling intepretation}
\label{sec-10_3}

$Av$ is simply scaled version of $v$ ($\lambda$ times); all components get magnified by the same amount
\subsection{Dynamic intepretation}
\label{sec-10_4}

For $Av=\lambda v$, if $\dot x= Ax,x(0)=v$ $\Rightarrow$ \fbox{$x(t) = e ^{\lambda t} v$} $= e ^{tA} v$.
\begin{itemize}
\item $A ^{2} v = \lambda ^{2} v$
\item So you just need a scalar in front of the $v$ to calculate $x(t)!$
\item \fbox{An eigenvector is an initial condition $x(0)$ for which the entire trajectory is really simple.}
\item solution $x(t) = e ^{\lambda t} v$ is a mode of $\dot x=Ax$ (associated with eigenvalue $\lambda$)
\end{itemize}
\subsection{Invariant set}
\label{sec-10_5}

a set $S \subseteq \mathbb{R}^{n}$ is \emph{invariant} under $\dot x = Ax$ if whenever $x(t) \in S$, then $x(\tau) \in S$ for all $\tau \ge t$ (you stay stuck within the set)
\begin{itemize}
\item vector field intepretation: trajectories only cut \emph{into} $S$
\end{itemize}
If a single point is an invariant set, it must be in the nullspace; $S=\{x_0\} \Leftrightarrow x_0 \in \mathcal{N}(A)$, so $Ax_0=0=\dot x$.
\begin{itemize}
\item line $\{tv | t \in \mathbb{R}\}$ is invariant for eigenvector $v$
\end{itemize}
\subsection{Complex eigenvectors}
\label{sec-10_6}

\begin{itemize}
\item for $a \in \mathbb C$, complex trajectory $a e ^{\lambda t} v$ satisfies $\dot x = Ax$, as well as \emph{real} part
\end{itemize}
$$
x(t) = \text{Re}(ae ^{\lambda t} v)
$$
$$
= e ^{\sigma t}
\begin{bmatrix}
  v _{re} & v _{im} \\
\end{bmatrix}
\begin{bmatrix}
  \cos \omega t  & \sin \omega t \\
  -\sin \omega t & \cos \omega t \\
\end{bmatrix}
\begin{bmatrix}
  \alpha \\
  -\beta \\
\end{bmatrix}
$$ 
where
$$
v= v _{re} + jv _{im} , \lambda = \sigma + j \omega, a = \alpha + j \beta
$$ 
\begin{itemize}
\item $\sigma$ gives logarithmic growth/decay factor
\item $\omega$ gives angular velocity of rotation in plane
\item trajectory stays in \emph{invariant plane} span $\{v _{re} ,v _{im}\}$
\end{itemize}
\subsection{Dynamic interpretation: left eigenvectors}
\label{sec-10_7}
\subsection{Summary:}
\label{sec-10_8}

\begin{itemize}
\item \emph{right eigenvectors} are initial conditions from which resulting motion is simple (i.e., remains on line or in plane)
\item \emph{left eigenvectors} give linear functions of state that are simple, for any initial condition
\end{itemize}
\subsection{Example- companion matrix}
\label{sec-10_9}

\begin{itemize}
\item Easy to get the characteristic polynomial
\item General truth: with these matrices you can't generally tell the system behavior by just looking at it
\item If you push a signal through an integrator, it gets less wiggly
\item By multiplying by the left eigenvector, you've filtered out the sinusoid?
\end{itemize}
\section{Lecture 13}
\label{sec-11}
\subsection{Example: Markov chain}
\label{sec-11_1}

Probability vector $p \in \mathbb{R}^{n}$ that you're in each of $n$ states: $p(t+1)=Pp(t)$. This probability evolves in time by being multiplied by state transition matrix $P$.
\begin{itemize}
\item $p _{i} (t)= {\bf Prob} (z(t)=i) \Rightarrow \sum ^{n} _{i=1} p _{i} (t) =1$
\item sum of each column is 1

\begin{itemize}
\item called stochastic
\end{itemize}

\item i.e., $[1\;1\; \cdots 1]$ is a left eigenvector of $P$ with $\lambda = 1$
\item so det($I-P$)=0, so there's also a nonzero right eigenvector s.t. $Pv=v$

\begin{itemize}
\item $v$ can always be chosen to have non-negative elements, and can be normalized
\end{itemize}

\item \textbf{Interpretation}: $v$ is an equilibrium distribution; you don't change your \emph{probability} distribution in time; always in $v$

\begin{itemize}
\item if $v$ unique, it's called the steady-state distribution of the Markov chain
\end{itemize}

\end{itemize}
\subsection{Diagonalization}
\label{sec-11_2}

\begin{itemize}
\item $v_1,...,v_n$ is LI set of eigenvectors of $A \in \mathbb{R}^{n\times n}$: $Av_i=\lambda_i v_i$
\item Concatenate in matrix language:
\end{itemize}
$$
A
\begin{bmatrix}
  v_1 & \cdots & v_n \\
\end{bmatrix}
=
\begin{bmatrix}
  v_1 & \cdots & v_n \\
\end{bmatrix}
\begin{bmatrix}
  \lambda_1 &           &        &           \\
            & \lambda_2 &        &           \\
            &           & \ddots &           \\
            &           &        & \lambda_n \\
\end{bmatrix}
$$
or, $AT=T\Lambda$, or $T ^{-1} AT=\Lambda$
\begin{itemize}
\item note, $T$ is invertible, since its columns are linearly independent
\item This is why, while $Av=\lambda v$ is more commonly used for a scalar eigenvalue, \fbox{$Av=v\lambda$} is more general, as it can represent a vector of eigenvalues $\lambda$.
\item so, $A$ is diagonalizable if

\begin{itemize}
\item $\exists$ $T$ s.t. $T ^{-1} AT=\Lambda$ is diagonal
\item $A$ has a set of linearly independent eigenvectors

\begin{itemize}
\item if $A$ not diagonalizable, it is called defective
\end{itemize}

\end{itemize}

\end{itemize}
\subsection{Not all matrices diagonalizable}
\label{sec-11_3}

i.e.,
$$
A=
\begin{bmatrix}
  0 & 1 \\
  0 & 0 \\
\end{bmatrix}
$$ 
\subsection{Distinct eigenvalues}
\label{sec-11_4}

\textbf{fact}: distinct eigenvalues in $A$ $\Rightarrow$ $A$ diagonalizable
\begin{itemize}
\item converse not true, i.e., $I \in \mathbb{R}^{7\times 7}$
\end{itemize}
\subsection{Diagonalization and left eigenvectors}
\label{sec-11_5}

rewrite $T ^{-1} AT = \Lambda$ as $T ^{-1} A = \Lambda T ^{-1}$:
$$ 
\begin{bmatrix}
  w ^{T} _1 \\
  \vdots    \\
  w ^{T} _n \\
\end{bmatrix}
A=\Lambda
\begin{bmatrix}
  w ^{T} _1 \\
  \vdots    \\
  w ^{T} _n \\
\end{bmatrix}
$$
\begin{itemize}
\item remember that $\Lambda$ is diagonal matrix, and multiplying by a diagonal matrix on the left is equivalent to scaling rows of the matrix

\begin{itemize}
\item on the right scales the columns
\end{itemize}

\end{itemize}
Remeber left/right multiplication results (whether it scales columns or rows) with $2 \times 2$ matrix multiplication:
$$
\begin{bmatrix}
    2 &   0 \\
    0 &   3 \\
\end{bmatrix}
\begin{bmatrix}
  x_1 & x_2 \\
  y_1 & y_2 \\
\end{bmatrix}
=
\begin{bmatrix}
  2x_1 & 2x_2 \\
  3y_1 & 3y_2 \\
\end{bmatrix}
$$
I.e., right multiplication of diagonal matrix scales the rows.
\begin{itemize}
\item Take LI set of eigenvectors as columns, invert that matrix, then the rows are \textbf{left} eigenvectors
\item An eigenvector is still an eigenvector after being scaled; so any can be normalized
\end{itemize}
\subsection{Modal form}
\label{sec-11_6}

Take a LI set of eigenvectors from $A$, shove them together as columns of new matrix $T$ = ``$A$ is diagonalizable by $T$''
\begin{itemize}
\item can define new coordinates by $x=T \tilde x$:
\item $\tilde x$ is coordinates of $x$ in the $T$ expansion; modal (or eigenvector) expansion

\begin{itemize}
\item $\tilde x$ is $x$ in terms of the eigenvectors
\end{itemize}

\end{itemize}
$$
T \dot{\tilde x}=AT \tilde x \Leftrightarrow \dot{ \tilde x}= T ^{-1} AT \tilde x \Leftrightarrow \dot{ \tilde x} = \Lambda \tilde x
$$
\begin{itemize}
\item in new coordinate system, system is diagonal (decoupled)
\item normally, with $\dot x=Ax$, there's a ton of cross-gains from input $x_i$ to output $y_j$, where all the outputs depend on all the inputs (assuming $A$ has only non-zero entries)

\begin{itemize}
\item \fbox{diagonalized system decouples it}; trajectory consists of $n$ independent modes:
\end{itemize}

\end{itemize}
$$
\tilde x_i (t) = e ^{\lambda_i t} \tilde x _{i} (0)
$$
\subsection{Real modal form}
\label{sec-11_7}

when eigenvalues ($\Rightarrow$ $T$) are complex
\begin{itemize}
\item notes show block diagram of complex mode (note if real parts $\sigma$ are removed, you get harmonic oscillator)
\end{itemize}
\subsection{Diagonalization simplification}
\label{sec-11_8}

Simplifies calculation of:
\begin{itemize}
\item resolvent
\item powers ($A^k$)
\item exponential ($e ^{A}=T {\bf diag} (e ^{\lambda_1},\dots, e ^{\lambda_n}) T ^{-1}$)
\item So, diagonalization is largely a conceptual tool, and sometimes gives great computational advantage
\end{itemize}
\subsection{Simplify for analytical functions of a matrix}
\label{sec-11_9}
\subsection{Solution via diagonalization}
\label{sec-11_10}

$\dot x=Ax$ solution is $x(t)=e ^{tA} x(0)$
\begin{itemize}
\item with diagonalization, solution given as
\end{itemize}
$$
x(t) = \sum ^{n} _{i=1} e ^{\lambda_i t} (w_i ^{T} x(0))v_i
$$ 
\subsection{Interpretation}
\label{sec-11_11}

\begin{itemize}
\item (left eigenvectors) decompose initial state $x(0)$ into modal components $w ^{T} _{i} x(0)$
\item $e ^{\lambda_i t}$ term propagates $i$ th mode forward $t$ seconds
\item reconstruct state as linear combination of (right eigenvectors)
\end{itemize}
\subsection{Application}
\label{sec-11_12}

Finding $x(0)$ that gives stable solution.
\subsection{Stability of discrete-time systems}
\label{sec-11_13}

\begin{itemize}
\item powers of complex numbers $s^k$ go to zero if $|s|<1$

\begin{itemize}
\item imaginary part tells how much of a rotation at each step you get
\end{itemize}

\item \fbox{$x(t+1) = Ax(t)$ is stable iff all eigenvalues of $A$ have magnitude less than one}
\item spectral radius of $A:\rho (A)= {\bf max} |\lambda _i |$

\begin{itemize}
\item so it is a stable system iff $\rho(A)<1$
\item $\rho$ gives rough growth or decay
\end{itemize}

\end{itemize}
\subsection{Jordan Canonical form}
\label{sec-11_14}

\begin{itemize}
\item \emph{Any} matrix $A \in \mathbb{R}^{n\times n}$ can be expressed in Jordan-canonical form (via `similarity transformation,' for some invertible matrix $T ^{-1}$)
\end{itemize}
$$
T ^{-1} AT=J=
\begin{bmatrix}
  J_1 &        &     \\
      & \ddots &     \\
      &        & J_q \\
\end{bmatrix}
$$ 
where
$$
J_i =
\begin{bmatrix}
  \lambda_i &         1 &        &           \\
            & \lambda_i & \ddots &           \\
            &           & \ddots &         1 \\
            &           &        & \lambda_i \\
\end{bmatrix}
\in \mathbb C ^{n_i \times n_i}
$$ 
\begin{itemize}
\item $J$ is `upper bidiagonal'
\item Jordan form is unique (up to permutations of blocks- blocks might be in different places in the diagonal)
\item \emph{Almost} strictly a conceptual tool; almost never used for numerical computations
\item Jordan forms are inutil if the matrix is already diagonalizable
\item When you get into Jordan form, you can use a chain of integrators to represent it in block diagram form
\item Jordan blocks refer to dynamics blocks that cannot be decoupled
\item \fbox{Jordan blocks yield:}

\begin{itemize}
\item repeated poles in resolvent
\item terms of form $t ^{p} e ^{t\lambda}$ in $e ^{tA}$: \fbox{solution to $\dot x=Ax$ is in form of polynomials, with $t^n e^{\lambda t}$ terms}
\end{itemize}

\end{itemize}
\subsection{Jordan block example}
\label{sec-11_15}

$$
\dot x=
\begin{bmatrix}
  0 & 1 &   &   &   \\
    & 0 & 1 &   &   \\
    &   & 0 & 1 &   \\
    &   &   & 0 & 1 \\
\end{bmatrix}
x
$$ 
\begin{itemize}
\item Eigenvalues are all zero
\item in the solutions you should expect $t ^{p} e^0$ (`constant') terms: solution of form $t + t ^{2} + t ^{3}$
\end{itemize}
\section{Lecture 14}
\label{sec-12}

\begin{itemize}
\item Jordan canonical form: `closest you can get to diagonalizable form, if matrix isn't diagonalizable'
\item via inverse Laplace transform:
\end{itemize}
$$
e ^{tJ_\lambda} = e ^{t \lambda} (I+t F_1 + \cdots + (t ^{k-1} /(k-1)!) F _{k-1})
$$

$$
= e ^{t \lambda}
\begin{bmatrix}
  1 & t & \cdots & t ^{k-1}/(k-1)! \\
    & 1 & \cdots & t ^{k-2}/(k-2)! \\
    &   & \ddots & \vdots          \\
    &   &        & 1               \\
\end{bmatrix}
$$

Jordan-blocks should be associated in your mind with polynomials times $e ^{\lambda t}$ 
\begin{itemize}
\item All FIR filters have Jordan blocks
\end{itemize}
\subsection{J-C application: Caley-Hamilton theorem}
\label{sec-12_1}

\begin{itemize}
\item if proving things about matrices, first show it for diagonalizable matrices, then with Jordan-Canonical form
\item for any $n\times n$ matrix $A$, the powers of $A: I,A ^{2}, A ^{3},...,A ^{n}$ span $\mathbb{R}^{n\times n}$

\begin{itemize}
\item $\mathcal X (A)=0$ (characteristic polynomial; by the time you get to $n$, you're getting dependent matrices)
\end{itemize}

\item wrong proof for evaluating characteristic polynomial $\mathcal X(s)= {\bf det} (sI-A)$ at $A$:

\begin{itemize}
\item $\mathcal X(A)= {\bf det} (AI-A)= {\bf det} (0)=0$
\item wrong, since definition of $\mathcal X$ involves $sI$ term, which is diagonal matrix (so $A$ can't be plugged into it)
\end{itemize}

\item correct example for
\end{itemize}
$$
A= \begin{bmatrix}
  1 & 2 \\
  3 & 4 \\
\end{bmatrix}
$$ 
\begin{eqnarray}
  \mathcal X &=& A ^{2} -5A-2I \\
&=&
\begin{bmatrix}
   7 & 10 \\
  15 & 22 \\
\end{bmatrix}
-5
\begin{bmatrix}
  1 & 2 \\
  3 & 4 \\
\end{bmatrix}
-2I \\
&=& 0
\end{eqnarray}
\begin{itemize}
\item \fbox{Caley-Hamilton theorem: the square of a (square) matrix is a linear combination of the Identity matrix and itself}

\begin{itemize}
\item the inverse is also a linear combination of $I$ and powers of $A$:
\item rewrite C-H theorem: $\mathcal X(A)= A ^{n} + a _{n-1} A ^{n-1} + \cdots + a _{0} I=0$ as

\begin{itemize}
\item $I=A\left( -(a_1 /a_0)I-(a_2 /a_0)A - \cdots -(1 /a_0) A ^{n-1} \right)$
\end{itemize}

\end{itemize}

\end{itemize}
$$
A ^{-1} =  -(a_1 /a_0)I-(a_2 /a_0)A - \cdots -(1 /a_0) A ^{n-1} 
$$ 
\begin{itemize}
\item Corollary: for every $p \in \mathbb Z_+$,
\end{itemize}
$$A ^{p} \in {\bf span} \{I,A,A ^{2} ...,A ^{n-1} \}$$
\subsection{Solving huge matrix equations}
\label{sec-12_2}

\begin{itemize}
\item usually have a method for fast matrix vector multiplication: $Az$
\item if you can do a quick $Az$ calculation, you can also do a quick $A ^{2} z,...A ^{n-1} z$ calculation

\begin{itemize}
\item using C-H thm, you can put scalars in front of them to get $A^{-1}$
\end{itemize}

\end{itemize}
\subsection{LTI system inputs \& outputs}
\label{sec-12_3}

LTI systems with input and outputs have the form $\dot x=Ax+Bu, y=Cx+Du$; $Ax$ is the \emph{drift} term, $Bu$ is the \emph{input} term. For 
\subsection{Interpretations}
\label{sec-12_4}

\begin{itemize}
\item state derivative $\dot x$ is the sum of the autonomous term $Ax$ and one term per input, $b_i u_i$

\begin{itemize}
\item if columns of $B$ are independent, each input $u_i$ gives another degree of freedom for $\dot x$
\item column $i$ of $B$ represents how much input $i$ affects $\dot x$
\item row $i$ of $B$ represents how much input $i$ affects the $i$ th component of $\dot x$
\end{itemize}

\item Good block diagram in notes, pg. 13-5
\end{itemize}
\subsection{Transfer matrix}
\label{sec-12_5}

\begin{itemize}
\item Solution to $\dot x=Ax+Bu$ via Laplace transform is \fbox{$\displaystyle x(t)= e ^{tA} x(0) + \int ^{t} _{0} e ^{(t-\tau) A} Bu (\tau) d\tau$ }

\begin{itemize}
\item $e ^{tA} x(0)$ is the (unforced or) autonomous response
\item $e ^{tA} B$ is the `input-to-state impulse matrix'

\begin{itemize}
\item integral term is convolution
\end{itemize}

\item $(sI-A) ^{-1} B$ is input-to-state \emph{transfer matrix}; the resolvent \texttimes{} $B$
\end{itemize}

\item Readout equation, $y=Cx+Du$ solution (via Laplacian)

\begin{itemize}
\item \fbox{$y(t) = C e ^{tA} x(0) + \int ^{t} _{0} C e ^{(t-\tau)A} B u(\tau) d\tau + Du(t)$}
\item transfer function: $H(s)= C(sI-A) ^{-1} B+D$
\item impulse matrix/impulse response: $h(t)= C e ^{tA} B + D \delta(t)$
\item zero initial condition gives $Y(s)= H(s)U(s), y=h*u$

\begin{itemize}
\item $H _{ij}$ is transfer function from input $u _{j}$ to output $y _{i}$
\end{itemize}

\end{itemize}

\item Mnemonic for convolution intuition $\displaystyle y(t) = \int^{t}_{0} h(t-\tau) u(\tau) d\tau$:

\begin{itemize}
\item output is `linear combination' or `mixture' of input in the past $u(\tau)$, weighted by how many seconds ago something happened
\item $\tau$ is `seconds ago'
\item example: if $h(7)$ is really big $\rightarrow$ output depends a \emph{lot} on the input 7 seconds ago

\begin{itemize}
\item remember, the $h$ term has been delayed and flipped: $h(-(\tau-t))$
\end{itemize}

\item if $h(t)$ decays, the system has \emph{fading memory}
\end{itemize}

\end{itemize}
\subsection{Impulse matrix}
\label{sec-12_6}

\begin{itemize}
\item $h(t) = C e ^{tA} B+D \delta(t), x(0)=0,y=h*u$
\end{itemize}
$$
y_i(t) = \sum ^{m} _{j=1} \int ^{t} _{0} h _{ij} (t-\tau) u _{j} (\tau) d\tau
$$ 
\begin{itemize}
\item for rain/river metaphor, where rain is input and river level is output, this matrix is summed over the input, with different regions of rainfall $u_j$ and different locations of the river where rainfall is measured, $y_i(t)$
\end{itemize}
\subsection{Step response}
\label{sec-12_7}

$$
s(t) = \int ^{t} _{0} h(\tau) d\tau
$$ 
\subsection{Spring dashpot example}
\label{sec-12_8}
\subsection{RC circuit example}
\label{sec-12_9}
\section{Lecture 15}
\label{sec-13}
\subsection{DC or static gain matrix}
\label{sec-13_1}

\begin{itemize}
\item transfer matrix at $s=0$ is $H(0)= -CA ^{-1} B + D \in \mathbb{R}^{m \times p}$
\item DC transfer matrix describes system under static conditions: $x,u,y$ constant

\begin{itemize}
\item $\Rightarrow y=H(0)u$
\item if system is stable, $\displaystyle H(0)= \int ^{\infty} _{0} h(t) dt= \lim_{t\rightarrow \infty} s(t)$
\end{itemize}

\end{itemize}
\subsection{Discretization with piecewise constant inputs}
\label{sec-13_2}
\subsection{Causality}
\label{sec-13_3}
\subsection{Idea of state}
\label{sec-13_4}

\begin{itemize}
\item $x(t)$ is state of system at time $t$:

\begin{itemize}
\item future output depends only on \emph{current state} and \emph{future output}

\begin{itemize}
\item sufficient statistic of what happened in the past to be able to predict the future
\end{itemize}

\item bridge between past inputs and future outputs

\begin{itemize}
\item `past and future are conditionally independent, given the state' (-machine learning )
\end{itemize}

\end{itemize}

\end{itemize}
\subsection{Change of coordinates}
\label{sec-13_5}

\begin{itemize}
\item start with LDS $\dot x=Ax+Bu,y=Cx+Du$
\item change coordinates in $\mathbb{R}^{n}$ to $\tilde x$ with $x=T \tilde x$
\end{itemize}
$$ 
\dot{\tilde x} = T ^{-1} \dot x = T ^{-1} (Ax+Bu) = T ^{-1} AT \tilde x + T ^{-1} Bu
$$
\begin{itemize}
\item LDS can be expressed as
\end{itemize}
$$
\dot{\tilde x} = \tilde A \tilde x+\tilde B u, y= \tilde C \tilde x + \tilde D u
$$
where
$$ 
\tilde A = T ^{-1} A T, \tilde B= T ^{-1} B, \tilde C = CT, \tilde D=D
$$ 
\begin{itemize}
\item You might want to do this to, i.e., get an $A$ with a bunch of zeros, since the system might be much easier to implement
\end{itemize}
\subsection{Standard forms for LDS}
\label{sec-13_6}

Can change coordinates to put $A$ in various forms (diagonal, real modal, Jordan \ldots{})
\subsection{Discrete-time systems}
\label{sec-13_7}

(Another block diagram in notes)
\subsection{Z-transform}
\label{sec-13_8}
\subsection{Discrete-time transfer function}
\label{sec-13_9}

\begin{itemize}
\item re-derivation of stuff we know, using the frequency domain
\end{itemize}
\section{Last Section, Singular Value Decomposition}
\label{sec-14}
\subsection{Eigenvalues of symmetric matrices}
\label{sec-14_1}

Given $A \in \mathbb{R}^{n\times n}:A=A ^{T}$ 
\begin{itemize}
\item eigenvalues always real
\item $Av=\lambda v, v\ne 0,v\in \mathbb C^n$; remember conjugate transpose, $\bar v ^{T}=v ^{H} = v ^{*T}$, or, in MATLAB,
\end{itemize}
\begin{verbatim}
 v'
\end{verbatim}

and remember, $\bar a a= |a| ^{2}$ 
$$
\Rightarrow v ^{H} Av= v ^{H} (Av) = \lambda v ^{H} v = \lambda \sum ^{n} _{i=1} |v_1| ^{2} 
$$ 
$$ 
...= \bar \lambda \sum ^{n} _{i=1} |v_1| ^{2} 
$$ 
so $\lambda = \bar \lambda \Rightarrow \lambda \in \mathbb{R}^{n}$
\subsection{Eigenvectors of symmetric matrices}
\label{sec-14_2}

\textbf{fact:} There is a set of orthonormal eigenvectors of $A$, $q_1,...,q_n: Aq_i=\lambda_i q_i, q_i ^{T} q_j = \delta _{ij}$ 
\begin{itemize}
\item In matrix form, $\exists$ orthogonal $Q: Q ^{-1} AQ = Q ^{T} AQ = \Lambda$
\item or, rewrite \emph{dyadic expansion of A}:
\end{itemize}
$$
A = Q \Lambda Q ^{T} = \sum ^{n} _{i=1} \lambda (q_i q_i ^{T})
$$
\begin{itemize}
\item \emph{engineering etiquette aside}: ``The eigenvectors of a symmetric matrix are orthonormal'' makes no sense; any matrix has zillions of eigenvectors

\begin{itemize}
\item but rather, ``you can choose the eigenvectors of a symmetric matrix to be orthonormal''
\end{itemize}

\end{itemize}
\subsection{Interpretations}
\label{sec-14_3}

Given $A=Q \Lambda Q ^{T}$. Remember, $Q ^{-1} =Q ^{T}$. (Notes have block diagram.)
This means, to multiply a vector $x$ by $A$, first multiply it by $Q^T$, then $\Lambda$, then $Q$
\begin{itemize}
\item first operation result $Q ^{T} x=Q ^{-1} x$ `resolves $x$ in the $q_i$ coordinates'
\item next: simple to multiply by diagonal matrix $\Lambda$ (simply scaling the matrix)
\item last: multiplying by $Q$ `reconstitutes the output'
\item \fbox{multiplying by $Q ^{T}, \Lambda, Q$ represents a `coding,' scaling, reconstruction operation }
\item Application: in JPEG, do DCT transformation, quantize in middle, then decode the image in last step
\item Geometrically:

\begin{itemize}
\item rotate by $Q^T$
\item dilation by $A$
\item rotate back by $Q$
\end{itemize}

\end{itemize}
Decomposition (review)
$$ 
A = \sum ^{n} _{i=1} \lambda _{i} q _{i} q _{i} ^{T} 
$$
expresses $A$ as linear combination of 1-dimensional projections.

If a matrix is real and symmetric, (and the eigenvalues of a matrix are distinct,) then any set of associated eigenvectors is orthogonal (and can be normalized).
$$ 
A v _{i} =\lambda _{i} v _{i}, \|v_i\| = 1
$$
Then,
$$ 
v _{i} ^{T} (A v _{j} ) = \lambda _{j} v _{i} ^{T} v _{j} = (A v _{i} ) ^{T} v _{j} = \lambda _{i} v _{i} ^{T} v _{j} 
$$
so $(\lambda_i - \lambda_j) v _{i} ^{T} v _{j} =0$ for $i\ne j, \lambda _{i} \ne \lambda _{j}$ \fbox{$\Rightarrow v _{i} ^{T} v _{j} =0$ }
So distinct eigenvectors of a symmetric real matrix are `orthogonal'
\subsection{Example: RC circuit}
\label{sec-14_4}
\section{Lecture 16}
\label{sec-15}
\subsection{Quadratic forms}
\label{sec-15_1}

Previously, we did mappings of form $y=Ax$, or $\dot x=Ax$. We're no longer doing problems with a linear structure.
\begin{itemize}
\item New form is \emph{quadratic form}
\end{itemize}
$$
f(x) = x ^{T} Ax= \sum ^{n} _{i,j=1} A _{ij} x _{i} x _{j} 
$$
\begin{itemize}
\item $(x ^{T} A x) ^{T} = x ^{T} A ^{T} x$
\item $x ^{T} Ax= x ^{T} ((A +A ^{T} )/2)x$ is another form of quadratic form; the middle part is the average of itself (member of $A$) and its associated trasposed self
\item weird etiquette: $A$ should be symmetric if you're dealing with quadratic forms

\begin{itemize}
\item often may want to symmetrize it to make it safe
\item i.e., this quadratic form
\end{itemize}

\end{itemize}
$$
x ^{T}
\begin{bmatrix}
  0 & 1 \\
  0 & 0 \\
\end{bmatrix}
x= x_1 x_2
$$

$$
x ^{T}
\begin{bmatrix}
     0 & 3/2 \\
  -1/2 & 0   \\
\end{bmatrix}
x= x_1 x_2
$$
But the canonical form would have 1/2 in the 2nd and 3rd entry (row-wise).
\begin{itemize}
\item If $A$ is diagonal in LDS, $y=Ax$, it means there is no cross-coupling. If $A$ is diagonal in a quadratic form, it gives you a weighted sum of squares.
\item \textbf{uniqueness:} if $x ^{T} Ax=x ^{T} Bx$ for all $x \in \mathbb{R}^{n}$ and $A=A^T, B = B ^{T}$, then $A=B$
\end{itemize}
\subsection{Examples of quadratic forms}
\label{sec-15_2}

\begin{itemize}
\item $\| B x \| ^{2} = x ^{T} (B ^{T} B)x; B ^{T} B is already symmetric$
\item $\sum ^{n-1} _{i=1} (x _{i+1} - x _{i} ) ^{2}$. This measures `wiggliness' of signal; sum of squares of difference from one to the next signal

\begin{itemize}
\item to get in quadratic form, multiply out: $x ^{2} _{i+1} + x ^{2} _{i} - 2 x _{i} x _{i+1}$:
\end{itemize}

\end{itemize}
$$
\begin{bmatrix}
  x_1 & x_2 & x_3 \\
\end{bmatrix}
\begin{bmatrix}
   1 & -1 &  0 \\
  -1 &  2 & -1 \\
   0 & -1 &  1 \\
\end{bmatrix}
\begin{bmatrix}
  x_1 \\
  x_2 \\
  x_3 \\
\end{bmatrix}
$$ 
\begin{itemize}
\item Zeros in corners of matrix, since there is no product between $x_1,x_3$
\item $\| Fx \| ^{2} - \| Gx \| ^{2}$

\begin{itemize}
\item $x ^{T} (F ^{T} F-G ^{T} G) x$
\end{itemize}

\item quadratic surface: level set: $\{ x| f(x) = a\}$
\item quadratic region: $\{ x|f(x) \le a \}$

\begin{itemize}
\item unit ball: $\{x | x ^{T} x \le 1\}$
\item unit sphere: $\{x | x ^{T} x = 1\}$
\end{itemize}

\end{itemize}
\subsection{Inequalities for quadratic forms}
\label{sec-15_3}

If $A = A ^{T}$, it is diagonalizable: $A = Q \Lambda Q ^{T}$ with sorted eigenvalues in $\Lambda$
\begin{eqnarray}
  x ^{T} Ax &=& x ^{T} Q \Lambda Q ^{T} x \\
   &=& (Q^Tx) ^{T} \Lambda(Q ^{T}x) \\
   &=& \sum ^{n} _{i=1} \lambda _{i} (q ^{T} _{i} x ) ^{2}  \\
   &\le& \lambda _{1} \sum ^{n} _{i=1} (q _{i} ^{T} x) ^{2}  \\
   &=& \lambda_1 \| x \| ^{2}\\
\end{eqnarray}
\begin{itemize}
\item So, how big can the quadratic form be?

\begin{itemize}
\item $x ^{T} Ax \le \lambda_1 x ^{T} x$
\end{itemize}

\item Another important inequality
\end{itemize}
$$
\lambda_n x ^{T} x \le x ^{T} Ax \le \lambda_1 x ^{T} x
$$
\begin{itemize}
\item $\lambda$$_1$ sometimes called $\lambda$ $_{\mathrm{max}}$, $\lambda$ $_{n}$ called $\lambda$ $_{\mathrm{min}}$
\item for \fbox{$A = A ^{T}$} $\in \mathbb{R}^{n\times n}$, $A$ is \emph{positive semidefinite} if $x ^{T} Ax \ge 0$ for all $x$

\begin{itemize}
\item notation: $A\ge0$
\item iff $\lambda _{min} (A) \ge 0$: all eigenvalues nonnegative
\item $A$ is \emph{positive definite} if $x ^{T} Ax >0$ for all $x\ne 0$
\end{itemize}

\item $A\ge 0 \Rightarrow A _{ij} \ge 0$
\item if $A$ not positive or negative [semi]definite, it is \emph{indefinite}
\item matrices \textbf{can be} \emph{incomparable}:

\begin{itemize}
\item $A \ngeq B$ \textbf{and} $B \ngeq A$, where it will depend on the $x$ (depend on the direction you're looking at it from)
\end{itemize}

\item $A \le B$ statement requires that both be square matrices, and probably symmetric (depending on social norms)

\begin{itemize}
\item if not symmetric, it means $\frac{A+A^T}{2} \le \frac{B+B^T}{2}$
\end{itemize}

\end{itemize}
\subsection{Ellipsoids}
\label{sec-15_4}

\begin{itemize}
\item if $A ^{T} > 0$, the set $\mathcal E = \{x | x ^{T} Ax \le 1 \}$ is an \emph{ellipsoid} in $\mathbb{R}^{n}$, centered at 0
\item semi-axes given by $s _{i} = \lambda _{i} ^{-1/2} q _{i}$

\begin{itemize}
\item eigenvectors determine directions of semiaxes
\item eigenvalues determine lengths of semiaxes
\item $\sqrt{\lambda_{max} / \lambda_{min}}$ gives maximum eccentricity
\end{itemize}

\item eigenvalue intuition in $\mathbb{R}^{3}$

\begin{itemize}
\item large pancake shape $\Rightarrow$ one large eigenvalue, two slightly smaller eigenvalues
\item long cigar shape $\Rightarrow$ two large eigenvalues, one small eigenvalue
\end{itemize}

\end{itemize}
\subsection{Gain of matrix in a direction}
\label{sec-15_5}

For $A \in \mathbb{R}^{m\times n}$ (not necessarily symmetric anymore), for $x \in \mathbb{R}^{n}, \|Ax\| / \|x \|$ gives \emph{amplification factor} or \emph{gain} of $A$ in direction $x$, and will vary with different $x$ directions
\begin{itemize}
\item Questions we will ask

\begin{itemize}
\item maximum gain of $A$

\begin{itemize}
\item \fbox{$\displaystyle \max _{x\ne 0} \frac{\|Ax\|}{\|x\|}$}
\item \emph{matrix norm} or \emph{spectral norm}, and denoted $\|A\|$ (overloading)
\item $\displaystyle \max _{x\ne 0} \frac{\|Ax\|}{\|x\|} = \max _{x \ne 0} \frac{x^TA^TAx}{\|x\|^2} = \lambda _{\max} (A ^{T} A)$
\item so, $\|A \|= \sqrt{\lambda _{\max} (A ^{T} A)}$
\end{itemize}

\item minimum gain of $A$

\begin{itemize}
\item $\displaystyle \min _{x\ne 0} \|Ax\| / \|x\| = \sqrt{\lambda _{\min} (A ^{T} A)}$
\end{itemize}

\item how does gain of $A$ vary with direction
\end{itemize}

\item Will give us a \emph{quantitative way} to talk about nullspace
\end{itemize}
\subsection{Example}
\label{sec-15_6}
\subsection{Properties of matrix norm}
\label{sec-15_7}

\begin{itemize}
\item triangle inequality
\item scaling
\item definiteness: $\|A\|= 0 \Leftrightarrow A = 0$
\item norm of a product inequality
\end{itemize}
\section{Lecture 17}
\label{sec-16}
\subsection{SVD}
\label{sec-16_1}

Singular value decomposition, for \emph{any} matrix $A$.
$$
A = U\Sigma V ^{T}
$$
where 
\begin{itemize}
\item $A \in \mathbb{R}^{m\times n}, {\bf Rank} (A) = r$
\item $A \in \mathbb{R}^{m\times r}, U ^{T} U = I$
\item $V \in \mathbb{R}^{n\times r}, V ^{T} V = I$
\item $\Sigma = {\bf diag} (\sigma _{1} ,..., \sigma _{r} )$, where $\sigma _{1} \ge ... \ge \sigma _{r} > 0$
\end{itemize}
\subsection{Dyadic expansion: ($U$ and $V$ have orthonormal columns)}
\label{sec-16_2}

$$
A = U \Sigma V ^{T} = \sum ^{r} _{i=1} \sigma _{i} u _{i} v _{i} ^{T} 
$$
\begin{itemize}
\item $\sigma _{i}$ are the nonzero \emph{singular values} of $A$
\item $v_i$ are the \emph{right} or \emph{input singular vectors} of $A$
\item $u _{i}$ are the \emph{left} or \emph{output singular vectors} of $A$
\end{itemize}
$$
A ^{T} A = (U \Sigma V ^{T} )^{T} (U \Sigma V ^{T}) = V \Sigma ^{2} V ^{T} 
$$
\begin{itemize}
\item $v _{i}$ are eigenvectors of $A ^{T}A$ corresponding to nonzero eigenvalues
\item $\sigma _{i} = \sqrt{\lambda _{i} (A ^{T} A) }$ (and $\lambda _{i} (A ^{T} A) = 0$ for $i>r$)
\item $\|A\| = \sigma _{1}$
\end{itemize}
$$
AA ^{T} = (U \Sigma V ^{T} ) (U \Sigma V ^{T} )^{T} = U \Sigma ^{2} U ^{T} 
$$
\begin{itemize}
\item $u _{i}$ are eigenvectors of $AA ^{T}$ corresponding to nonzero eigenvalues
\item $\sigma _{i} = \sqrt{\lambda _{i} (AA ^{T}) }$ (and $\lambda _{i} (AA ^{T}) = 0$ for $i>r$)
\item $u _{1} ,..., u _{r}$ are orthonormal basis for range($A$)
\item $v _{1} ,..., v _{r}$ are orthonormal basis for $\mathcal{N}(A) ^{\perp}$
\end{itemize}
\subsection{Interpretations}
\label{sec-16_3}

$A = U \Sigma V ^{T}$

linear mapping $y=Ax$ can be decomposed as
\begin{itemize}
\item compute coefficients of $x$ along input directions $v_1,...,v_r$
\item scale coefficients by $\sigma _{i}$
\item reconstitute along output directions $u _{1} ,..., u _{r}$
\item $u _{1}$ is highest gain output direction
\item $v_1$ is most sensitive (highest gain) input direction
\item $A v_1 = \sigma_1 u_1$
\end{itemize}

SVD gives clearer picture of gain as function of input/output directions.
\subsection{General pseudo-inverse}
\label{sec-16_4}

\begin{itemize}
\item $A\ne 0\Rightarrow A=U\Sigma V ^{T}$
\end{itemize}
General pseudo-inverse: \fbox{$A ^{\dagger} = V \Sigma ^{-1} U ^{T}$} is the pseudo-inverse of $A$.

If $A$ is skinny and full rank,
$$
A ^{\dagger} = (A ^{T} A) ^{-1} A ^{T} 
$$
If $A$ is fat and full rank,
$$
A ^{\dagger} = A ^{T} (AA ^{T}) ^{-1} 
$$
In the general case, (if $A$ is not full-rank)
$$
X _{ls} = \{ z: \|Az-y\|= \min _{w} \|Aw-y\| \}
$$
is the set of least squares solutions.

$x _{pinv} = A ^{\dagger} y\in X _{ls}$ has minimum norm on $X _{ls}$, so this is the minimum-norm, least squares solution
\subsection{Pseudo-inverse via regularization}
\label{sec-16_5}

Minimizer of $\|Ax-y\| ^{2} + \mu \|x\| ^{2}$ is given as
$$
x _{\mu} = (A ^{T} A + \mu I) ^{-1} A ^{T} y
$$
\section{Lecture 18}
\label{sec-17}
\subsection{Sensitivity of linear equations to data error}
\label{sec-17_1}

To perturbations $\delta x,\delta y$. Looking at \emph{relative error} gives us
$$
\frac{\|\delta x\|}{\|x\|} \le \|A\| \|A ^{-1} \| \frac{\| \delta y \|}{\|y\|} 
$$

$$
\kappa(A) = \text{ cond}(A) = \|A \| \|A ^{-1} \| = \sigma _{\max} (A) /\sigma _{\min} (A)
$$
So, the relative error in solution $x \le$ condition number $\cdot$ relative error in data $y$. In terms of \# of bits of guaranteed accuracy, `\# of bits accuracy in solution $\approx$ \# bits accuracy in data $-\log _{2} \kappa$.'
\subsection{Low rank applications}
\label{sec-17_2}

If you want to decompose $A$ into $A=BC$, you can remember $QR$ factorization (does skinny-fat factorization), but we'd also want an $A$ that is approximated by such a factorization. Suppose $A \in \mathbb{R}^{m\times n}, {\bf Rank} (A)= r$ with SVD $A=U\Sigma V ^{T} = \sum _{i=1} ^{r} \sigma _{i} u _{i} v _{i} ^{T}$, and we want $\hat A: {\bf Rank} (\hat A) \le p<r: \|A-\hat A\|$ is minimized.

The optimal rank $p$ approximator of $A$ is
$$
\hat A=\sum ^{p} _{i=1} \sigma _{i} u _{i} v _{i} ^{T} 
$$
\begin{itemize}
\item $\|A-\hat A\|= \| \sum ^{r} _{i=p+1} \sigma _{i} u _{i} v _{i} ^{T} \| = \sigma _{p+1}$
\item SVD dyads $u _{i} v _{i} ^{T}$ are ranked in order of `importance'; take $p$ to get rank $p$ approximant
\item keep in mind Frobenius norm $\|A-\hat A\| _{F} = \| A(i)- \hat A(i)\| = \sqrt{\sum _{i,j} (A _{ij} - \hat A _{ij} ) ^{2} }$
\item If you have a set of 100 $\mathbb{R}^{10}$ prices that has dimension=3, it means there are 3 underlying factors; this would give you `stunning' predictive power
\item You would \emph{almost} (i.e., practically) have a dimension 3 matrix if the SVD $\Sigma$ had 3 singular values that were significantly larger than the rest
\end{itemize}
\subsection{State transfer}
\label{sec-17_3}
\subsection{Reachability}
\label{sec-17_4}

Consider state transfer from $x(0)= 0$ to $x(t)$ 
\begin{itemize}
\item $x(t)$ is reachable in $t$ seconds
\item define $\mathcal R _{t} \subseteq \mathbb{R}^{n}$ as the set of points reachable in $t$ seconds for CT system $\dot x=Ax+Bu$,
\end{itemize}
$$
\mathcal R _{t} = \left\{\int ^{t} _{0} e ^{(t-\tau)A} Bu(\tau) d\tau \big| u:[0,t] \rightarrow \mathbb{R}^{m} \right\}
$$
and for DT system $x(t+1)= Ax(t) + Bu(t)$,
$$
\mathcal R _{t} = \left\{\sum ^{t-1} _{\tau=0} A ^{t-1-\tau} Bu(\tau) \big| u(t) \in \mathbb{R}^{m} \right\}
$$
If a state is reachable in 1 s, it's reachable in 2 s (just leave it at zero for the first second) if you're starting from zero initial state (in both CT and DT systems).
\subsection{Reachability for discrete-time LDS}
\label{sec-17_5}

DT system $x(t+1)= Ax(t)+Bu(t), x(t) \in \mathbb{R}^{n}$
$$
x(t)= \mathcal C_t
\begin{bmatrix}
  u(t-1) \\
  \vdots \\
  u(0)   \\
\end{bmatrix}
$$
where $\mathcal C_t = [B \; Ab \; \cdots \; A ^{t-1} B]$. So reachable set at $t$ is $\mathcal{R}_t = \text{range} (\mathcal C_t)$

\fbox{If we want to see if a state is reachable in 1 step, see if it's in the range of $B$.}

\fbox{If we want to see if it's reachable in 2 steps, see if it's in the range of $B,AB$.}
Caley-Hamilton theorem lets us express each $A ^{k}$ for $x\ge n$ as linear combination of $A ^{0} ,...,A ^{n-1}$.
\section{Lecture 19}
\label{sec-18}

But, you can't increase your reach after $t \ge n$ seconds (or epochs). I.e.,
$$
\mathcal R_t = \left\{
\begin{matrix}
  {\bf range} (\mathcal C_t) & \quad t<n    \\
  {\bf range} (\mathcal C)   & \quad t\ge n \\
\end{matrix}
\right.
$$
where $\mathcal C=\mathcal C _{n}$ is the \emph{controllability matrix}.
\subsection{General state transfer}
\label{sec-18_1}

$$
x(t _{f}) = A ^{t_f-t_i} x(t_i) + \mathcal C_{t_f-t_i}
\begin{bmatrix}
  u(t_f-1) \\
  \vdots   \\
  u(t_i)   \\
\end{bmatrix}
$$
\begin{itemize}
\item First term on rhs is what would happen without any input (the drift term)
\end{itemize}
so we can transfer $x(t_i)$ to $x(t_f)=x _{des}$ iff
$$
x _{des} -A ^{t_f-t_i} x(t_i) \in \mathbb{R} _{t_f-t_i} 
$$
\begin{itemize}
\item we're not hoping $x_{des}$ is reachable, we want it minus the drift

\begin{itemize}
\item So it is possible to be able to reach a state in 4 steps, but not in 5
\end{itemize}

\item This reduces to the reachability problem
\item if system is controllable, any state transfer can be achieved in $\le$ \emph{n} steps
\item important special case: driving state to zero
\end{itemize}
\subsection{Minimum time control problem}
\label{sec-18_2}

Given
$$
x(t+1) = Ax(t) + Bu(t), \quad x(0) = x_0
$$
how do you minimize $t$? Check if $A ^{t} x_0 \in \mathcal R ([B \; \cdots \; A ^{t-1} B])$ 
\subsection{Least-norm input for reachability}
\label{sec-18_3}

assume system is reachable, ${\bf Rank} (\mathcal C_t) = n$ to steer $x(0)=0$ to $x(t)= x _{des}$, inputs $u(0),...,u(t-1)$ to satisfy
$$
x _{des} = \mathcal C _{t}
\begin{bmatrix}
  u(t-1) \\
  \vdots \\
  u(0)   \\
\end{bmatrix}
$$
among all $u$ that steer $x(0)= 0$ to $x(t)= x _{des}$, the one that minimizes
$$
\sum ^{t-1} _{\tau=0} \| u(\tau) \| ^{2}
$$
The least-square solution for the input is given by
$$
\begin{bmatrix}
  u _{ln}(t-1) \\
  \vdots       \\
  u _{ln}(0)   \\
\end{bmatrix}
= \mathcal C ^{T} _{t} (\mathcal C _{t} \mathcal C^{T} _{t} ) ^{-1} x _{des} 
$$
\subsection{Stability}
\label{sec-18_4}
\subsection{Continuous time reachability}
\label{sec-18_5}

consider $\dot x = Ax+Bu$ with $x(t) \in \mathbb{R}^{n}$

reachable set at time $t$ is
$$
\mathcal R _{t} = \left\{ \int ^{t} _{0} e ^{(t-\tau)A} Bu(\tau) d\tau \left| u: \right. [0,t] \rightarrow \mathbb{R}^{m} \right\}
$$
for $t>0$, $\mathcal R _{t} = \mathcal R = {\bf range} (\mathcal C)$ where
$$
\mathcal C = [B \; AB \; \cdots \; A ^{n-1} B]
$$
is the controllability matrix of $(A,B)$
\begin{itemize}
\item According to the model, you can hit anything infinitely fast
\end{itemize}
\subsection{Impulsive inputs}
\label{sec-18_6}
\subsection{Example}
\label{sec-18_7}
\subsection{Least-norm input for reachability}
\label{sec-18_8}

Assume that $\dot x=Ax+Bu$ is reachable. We want $u$ that steers $x(0)=0$ to $x(t)=x _{des}$ and minimizes
$$
\int ^{t} _{0} \| u(\tau) \| ^{2} d\tau
$$
let's discretize system with interval $h=t/N$ (eventually let $N\rightarrow \infty$). So $u$ is piecewise constant:
$$
u(\tau) = u _{d} (k) \text{ for } xh \le \tau < (k+1)h,\quad k=0,...,N-1
$$
\ldots{}Get long expression for least norm solution for $u$ 
\section{Lecture 20}
\label{sec-19}
\subsection{Observability and state estimation}
\label{sec-19_1}

Consider DT system
$$
x(t+1) = Ax(t) + Bu(t) + w(t), \quad y(t)= Cx(t) +Du(t) + v(t)
$$
\begin{itemize}
\item $w$ is state disturbance/noise
\item $v$ is sensor noise/error
\item state estimation problem: estimate $x(s)$ from
\end{itemize}
$$
u(0),...,u(t-1), \quad y(0), ..., y(t-1)
$$
\begin{itemize}
\item $s=0$: estimate initial state
\item $s=t-1$: estimate current state
\item $s=t$: estimate/predict next state
\item estimate $\tilde x(s)$, called \emph{observer} or \emph{state estimator}
\end{itemize}
\subsection{Noiseless case}
\label{sec-19_2}

Find $x(0)$, with no state or measurement noise
$$
x(t+1) = Ax(t) + Bu(t), \quad y(t)= Cx(t) + Du(t)
$$
then we have
$$
\begin{bmatrix}
  y(0)   \\
  \vdots \\
  y(t-1) \\
\end{bmatrix}
=\mathcal O _{t} x(0) + \mathcal T _{t}
\begin{bmatrix}
  u(0)   \\
  \vdots \\
  u(t-1) \\
\end{bmatrix}
$$


where
$$
\mathcal O_t =
\begin{bmatrix}
  C         \\
  CA        \\
  \vdots    \\
  CA ^{t-1} \\
\end{bmatrix}
\quad
\mathcal T _{t} =
\begin{bmatrix}
  D          & 0          & \cdots &        &   \\
  CB         & D          & 0      & \cdots &   \\
  \vdots     &            &        &        &   \\
  CA ^{t-2}B & CA ^{t-3}B & \cdots & CB     & D \\
\end{bmatrix}
$$
\begin{itemize}
\item $\mathcal O _t$ maps initial state into resulting output over $[0,t-1]$
\item $\mathcal T_t$ maps input to output over $[0,t-1]$
\end{itemize}
\subsection{Observability matrix}
\label{sec-19_3}
\subsection{Least-squares observers}
\label{sec-19_4}
\subsection{Parting thoughts}
\label{sec-19_5}
\subsubsection{Linear algebra}
\label{sec-19_5_1}
\subsubsection{Levels of understanding}
\label{sec-19_5_2}

\begin{itemize}
\item High school: 17 variables, 17 equations $\rightarrow$ usually has unique solution

\begin{itemize}
\item 80 vars, 60 eqns $\rightarrow$ probably 20 extra degrees of freedom
\end{itemize}

\item Platonic view (`math')

\begin{itemize}
\item singular, rank, range, nullspace, Jordan form, controllability
\item stuff is true or false
\end{itemize}

\item Quantitative

\begin{itemize}
\item based on least-squares, SVD
\item gives numerical measures for ideas like singularity, rank, etc
\item interpretation depends on (practical) context
\item very useful in practice
\end{itemize}

\end{itemize}
\section{Appendix}
\label{sec-20}

Some special things to remember.
\subsection{Inverse, transpose properties}
\label{sec-20_1}

\begin{itemize}
\item $(AB) ^{-1} = B ^{-1} A ^{-1}$
\item $(A ^{-1}) ^{T} = (A ^{T}) ^{-1}= A ^{-T}$
\end{itemize}
\subsection{Invertibility implications}
\label{sec-20_2}

For an $n$-by-$n$ matrix $A$ 

\begin{center}
\begin{tabular}{ll}
 Invertible                                &  mnemonic                                                                                       \\
\hline
 $\vert A\vert \ne 0$                      &  $\vert A\vert = 0$ $\Rightarrow$ you can't compute the inverse                                 \\
                                           &  - (remember base case 2 \texttimes{} 2 matrix inverse involves $1/\vert A\vert$ term)          \\
 non-singular                              &  singular $\Rightarrow$ the matrix sends a nontrivial subspace to the singular subspace, \{0\}  \\
 $A$ is full rank                          &  linearly independent columns (invertibility $\Rightarrow$ 1-to-1/injective)                    \\
 $\mathcal{N}(A)=\{0\}$                    &  linearly independent columns                                                                   \\
 $\mathcal R (A)= \mathbb{R}^{n}$          &  linearly independent columns                                                                   \\
 $Ax=b$ has unique solution for every $b$  &  - no more than one solution (can't add members of $\mathcal N (A)$ for multiple $b$)           \\
                                           &  - one solution, since $\mathcal R (A)= \mathbb{R}^{n}$; everything reachable/surjective        \\
                                           &  - one solution found using the unique inverse of \emph{A}                                      \\
 rref($A)=I_n$                             &                                                                                                 \\
 $A$ is a product of elementary matrices   &                                                                                                 \\
\end{tabular}
\end{center}
\subsection{Tips}
\label{sec-20_3}

\begin{itemize}
\item to sum up all the elements in matrix $A$, multiply by one vectors: $\displaystyle \sum _{i,j} (A) _{i,j} = {\bf 1} ^{T} A {\bf 1}$
\item each row $i$ in $Z$ is a linear combination of rows $i,...,n$ in $Y: Z=UY$, where $U$ is upper triangular: $U _{ij} =0$ for $i>j$

\begin{itemize}
\item more in 2.24, hw2
\end{itemize}

\item $E _{i,j} = e _{i} e _{j} ^{T} \in \mathbb{R}^{n\times n}, i,j=1,...,n$ is $n\times n$ matrix with a 1 in $i,j$ th entry, and zero elsewhere

\begin{itemize}
\item has $n^2$ dimensions
\end{itemize}

\item When you do a polynomial of a similarity matrix, you can pull out the outer matrices: $\mathcal X(T\Lambda T ^{-1} ) = T \mathcal X(\Lambda) T ^{-1}$
\item If $AB=0$, and each is full rank, (neither is zero), then $\mathcal R (B) \subseteq \mathcal N (A)$
\item $A ^{k} = 0, k\ge 1\Rightarrow$ eigenvalues of $A$ are zero. If $A$ is diagonalizable, then $A ^{k}$ can be expressed as $T \Lambda ^{k} T ^{-1}$, where $\Lambda$ is diagonal, consisting of the eigenvalues multiplied by the $k$ th power, since $T$ is non-singular.

\begin{itemize}
\item ${\bf Rank}(A)=r \Rightarrow A \in \mathbb{R}^{n\times n}$ has exactly $r$ non-zero eigenvalues
\end{itemize}

\item Every eigenvalue of $AB$ is an eigenvalue of $BA$, if $A,B \in \mathbb{R}^{n\times n}$: Given $\lambda$ is eigenvalue of $AB:ABv=\lambda v, BA(Bv)=B(AB)v=\lambda Bv$. Thus $Bv$ is an eigenvector of $BA$ assocated with $\lambda$
\item To check whether \fbox{$x\in \mathcal R(A)$}, check whether \fbox{$rank([A \;x])= rank(A)$ }
\item Least squares approximation that minimizes $x$ in $Ax=y, x _{ls} = (A ^{T} A) ^{-1} A ^{T} y$ is given in MATLAB by \texttt{xls=A\textbackslash{}y;}

\begin{itemize}
\item only works if $A$ is skinny or square
\item compute using QR (economy) factorization with
\begin{verbatim}
     [Q,R]=qr(A,0); % compute economy QR decomposition
     xls=R\(Q'*y);
\end{verbatim}

\end{itemize}

\item Least norm solution, $x _{ln} = A ^{T} (AA ^{T} ) ^{-1} y$, can be computed by \texttt{xln=A'*inv(A*A')*y;}

\begin{itemize}
\item $A$ cannot be full rank or skinny
\item via QR factorization:
\begin{verbatim}
     [Q,R]=qr(A',0);
     xln=Q*(R'\y);
\end{verbatim}


\begin{itemize}
\item for more, see \href{file:///Users/FingerMan/Dropbox/AK-MBP/edu/systems/IntroToLinearDynamicalSys/materials/lsoeldsee263/Additional5-sle_matlab.pdf}{here}
\end{itemize}

\end{itemize}

\end{itemize}
\subsection{Unrelated stuff}
\label{sec-20_4}
\subsubsection{Series}
\label{sec-20_4_1}

\begin{eqnarray}
 S &=&\sum ^{N} _{k=0} a ^{k} =a ^{0} + a ^{1} + ...+ a ^{N} \\
 aS &=&\sum ^{N} _{k=0} a ^{k} =a ^{1} + a ^{1} + ...+ a ^{N+1} \\
 aS-S &=& -1+a ^{N+1} \\
 S &=& \frac{a^{N+1}-1}{a-1}
\end{eqnarray}
\subsubsection{Taylor/Mclauren}
\label{sec-20_4_2}

Intuition for approximating a function at a point (say at $x=0$): assuming we know $f(0),f'(0),f''(0),f'''(0)$, put into polynomials
\begin{eqnarray}
  p_0(x) &=& f(0) \\
  p_1(x) &=& f(0) + f'(0)x \\
  p_2(x) &=& f(0) + f'(0)x + \frac{1}{2} f''(0)x^2 \\
  p_3(x) &=& f(0) + f'(0)x + \frac{1}{2} f''(0)x^2 + \frac{1}{2} \frac{1}{3} f'''(0) x^3 \\
\end{eqnarray}
Our polynomial $p_n(x)$ will both have the same value at $x=0$, as well as the same \emph{n} th derivatives when evaluated at 0. I.e.,
\begin{eqnarray}
  p_2'(x) &=& f'(0) + f''(0) x \\
  p_2'(0) &=& f'(0) \\
\end{eqnarray}
In general, the $n$ th order McLauren approximation polynomial of $f(x)$ at point $x_1$ will be given as
$$
f(x) \approx f(x_1) + f'(x_1)x + \frac{1}{2} f''(x_1) x ^{2} + \frac{1}{2\cdot 3} f ^{(3)} x ^{3} + ... + \frac{1}{n!} f ^{(n)} x ^{n} 
$$ 
In general, the $n$ th order Taylor approximation polynomial of $f(x)$ at point $c$ will be given as
$$
f(x) \approx f(c) + f'(c)(x-c) + \frac{1}{2} f''(c) (x-c) ^{2} + ... + \frac{1}{n!} f ^{(n)} (x-c) ^{n} 
$$ 
So when you evaluate derivatives at $x=c$, the higher order terms will still drop to zero, leaving you with the exact derivative value.
\begin{itemize}

\item Cosine McLauren (around 0)\\
\label{sec-20_4_2_1}%
$$
\cos (x) = 1- \frac{x^2}{2!} + \frac{x^4}{4!} - \frac{x^6}{6!} + ...
$$ 

\item Sine McLauren (around 0)\\
\label{sec-20_4_2_2}%
$$
\sin(x) = x- \frac{x^3}{3!} + \frac{x^5}{5!} - \frac{x^7}{7!} + ...
$$ 

\item e McLauren (around 0)\\
\label{sec-20_4_2_3}%
$$
e ^{x} = 1+x + \frac{x^2}{2!} + \frac{x^3}{3!} + ...
$$ 
\end{itemize} % ends low level
\section{Homework assignments}
\label{sec-21}


\begin{center}
\begin{tabular}{lll}
 \href{file:///Users/FingerMan/Desktop/Engineering/eng-control-and-cv/stanford hw/hw1sol.pdf}{Homework 1}  &  Lecture 4   &  2.1–2.4, 2.6, 2.9, 2.12, +                                                \\
 \href{file:///Users/FingerMan/Desktop/Engineering/eng-control-and-cv/stanford hw/hw2sol.pdf}{Homework 2}  &  Lecture 6   &  3.2, 3.3, 3.10, 3.11, 3.16, 3.17, +                                       \\
 \href{file:///Users/FingerMan/Desktop/Engineering/eng-control-and-cv/stanford hw/hw3sol.pdf}{Homework 3}  &  Lecture 8   &  2.17, 3.13, 4.1–4.3, 5.1, 6.9, +                                          \\
 \href{file:///Users/FingerMan/Desktop/Engineering/eng-control-and-cv/stanford hw/hw4sol.pdf}{Homework 4}  &  Lecture 10  &  5.2, 6.2, 6.5, 6.12, 6.14, 6.26, 7.3, 8.2                                 \\
 \href{file:///Users/FingerMan/Desktop/Engineering/eng-control-and-cv/stanford hw/hw5sol.pdf}{Homework 5}  &  Lecture 13  &  10.2, 10.3, 10.4, +                                                       \\
 \href{file:///Users/FingerMan/Desktop/Engineering/eng-control-and-cv/stanford hw/hw6sol.pdf}{Homework 6}  &  Lecture 14  &  9.9, 10.5, 10.6, 10.8, 10.14, 11.3, and 11.6a                             \\
 \href{file:///Users/FingerMan/Desktop/Engineering/eng-control-and-cv/stanford hw/hw7sol.pdf}{Homework 7}  &  Lecture 16  &  10.9, 10.11, 10.19, 11.13, 12.1, 13.1, +                                  \\
 \href{file:///Users/FingerMan/Desktop/Engineering/eng-control-and-cv/stanford hw/hw8sol.pdf}{Homework 8}  &  Lecture 18  &  13.17, 14.2, 14.3, 14.4, 14.6, 14.8, 14.9, 14.11, 14.13, 14.21, 14.33, +  \\
 \href{file:///Users/FingerMan/Desktop/Engineering/eng-control-and-cv/stanford hw/hw9sol.pdf}{Homework 9}  &  Lecture 20  &  14.16, 14.26, 15.2, 15.3, 15.6, 15.8, 15.10, and 15.11                    \\
\end{tabular}
\end{center}

\end{document}